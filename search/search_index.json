{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NERDA Not only is NERDA a mesmerizing muppet-like character. NERDA is also a python package, that offers a slick easy-to-use interface for fine-tuning pretrained transformers for Named Entity Recognition (=NER) tasks. You can also utilize NERDA to access a selection of precooked NERDA models, that you can use right off the shelf for NER tasks. NERDA is built on huggingface transformers and the popular pytorch framework. Installation guide NERDA can be installed from PyPI with pip install NERDA If you want the development version then install directly from GitHub . Named-Entity Recogntion tasks Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. 1 Example Task: Task Identify person names and organizations in text: Jim bought 300 shares of Acme Corp. Solution Named Entity Type 'Jim' Person 'Acme Corp.' Organization Read more about NER on Wikipedia . Train Your Own NERDA Model Say, we want to fine-tune a pretrained Multilingual BERT transformer for NER in English. Load package. from NERDA.models import NERDA Instantiate a NERDA model ( with default settings ) for the CoNLL-2003 English NER data set. from NERDA.datasets import get_conll_data model = NERDA(dataset_training = get_conll_data('train'), dataset_validation = get_conll_data('valid'), transformer = 'bert-base-multilingual-uncased') By default the network architecture is analogous to that of the models in Hvingelby et al. 2020 . The model can then be trained/fine-tuned by invoking the train method, e.g. model.train() Note : this will take some time depending on the dimensions of your machine (if you want to skip training, you can go ahead and use one of the models, that we have already precooked for you in stead). After the model has been trained, the model can be used for predicting named entities in new texts. # text to identify named entities in. text = 'Old MacDonald had a farm' model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']]) This means, that the model identified 'Old MacDonald' as a PER son. Please note, that the NERDA model configuration above was instantiated with all default settings. You can however customize your NERDA model in a lot of ways: Use your own data set (finetune a transformer for any given language) Choose whatever transformer you like Set all of the hyperparameters for the model You can even apply your own Network Architecture Read more about advanced usage of NERDA in the detailed documentation . Use a Precooked NERDA model We have precooked a number of NERDA models for Danish and English, that you can download and use right off the shelf. Here is an example. Instantiate a multilingual BERT model, that has been finetuned for NER in Danish, DA_BERT_ML . from NERDA.precooked import DA_BERT_ML() model = DA_BERT_ML() Down(load) network from web: model.download_network() model.load_network() You can now predict named entities in new (Danish) texts # (Danish) text to identify named entities in: # 'Jens Hansen har en bondeg\u00e5rd' = 'Old MacDonald had a farm' text = 'Jens Hansen har en bondeg\u00e5rd' model.predict_text(text) ([['Jens', 'Hansen', 'har', 'en', 'bondeg\u00e5rd']], [['B-PER', 'I-PER', 'O', 'O', 'O']]) List of Precooked Models The table below shows the precooked NERDA models publicly available for download. Model Language Transformer Dataset F1-score DA_BERT_ML Danish Multilingual BERT DaNE 82.8 DA_ELECTRA_DA Danish Danish ELECTRA DaNE 79.8 EN_BERT_ML English Multilingual BERT CoNLL-2003 90.4 EN_ELECTRA_EN English English ELECTRA CoNLL-2003 89.1 F1-score is the micro-averaged F1-score across entity tags and is evaluated on the respective test sets (that have not been used for training nor validation of the models). Note, that we have not spent a lot of time on actually fine-tuning the models, so there could be room for improvement. If you are able to improve the models, we will be happy to hear from you and include your NERDA model. Model Performance The table below summarizes the performance (F1-scores) of the precooked NERDA models. Level DA_BERT_ML DA_ELECTRA_DA EN_BERT_ML EN_ELECTRA_EN B-PER 93.8 92.0 96.0 95.1 I-PER 97.8 97.1 98.5 97.9 B-ORG 69.5 66.9 88.4 86.2 I-ORG 69.9 70.7 85.7 83.1 B-LOC 82.5 79.0 92.3 91.1 I-LOC 31.6 44.4 83.9 80.5 B-MISC 73.4 68.6 81.8 80.1 I-MISC 86.1 63.6 63.4 68.4 AVG_MICRO 82.8 79.8 90.4 89.1 AVG_MACRO 75.6 72.8 86.3 85.3 'NERDA'? ' NERDA ' originally stands for 'Named Entity Recognition for DAnish' . However, this is somewhat misleading, since the functionality is no longer limited to Danish. On the contrary it generalizes to all other languages, i.e. NERDA supports fine-tuning of transformers for NER tasks for any arbitrary language. Background NERDA is developed as a part of Ekstra Bladet \u2019s activities on Platform Intelligence in News (PIN). PIN is an industrial research project that is carried out in collaboration between the Technical University of Denmark , University of Copenhagen and Copenhagen Business School with funding from Innovation Fund Denmark . The project runs from 2020-2023 and develops recommender systems and natural language processing systems geared for news publishing, some of which are open sourced like NERDA . Shout-outs Thanks to Alexandra Institute for with the danlp package to have encouraged us to develop this package. Thanks to Malte H\u00f8jmark-Bertelsen and Kasper Junge for giving feedback on NERDA . Contact We hope, that you will find NERDA useful. Please direct any questions and feedbacks to us ! If you want to contribute (which we encourage you to), open a PR . If you encounter a bug or want to suggest an enhancement, please open an issue .","title":"Home"},{"location":"#nerda","text":"Not only is NERDA a mesmerizing muppet-like character. NERDA is also a python package, that offers a slick easy-to-use interface for fine-tuning pretrained transformers for Named Entity Recognition (=NER) tasks. You can also utilize NERDA to access a selection of precooked NERDA models, that you can use right off the shelf for NER tasks. NERDA is built on huggingface transformers and the popular pytorch framework.","title":"NERDA "},{"location":"#installation-guide","text":"NERDA can be installed from PyPI with pip install NERDA If you want the development version then install directly from GitHub .","title":"Installation guide"},{"location":"#named-entity-recogntion-tasks","text":"Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. 1","title":"Named-Entity Recogntion tasks"},{"location":"#example-task","text":"Task Identify person names and organizations in text: Jim bought 300 shares of Acme Corp. Solution Named Entity Type 'Jim' Person 'Acme Corp.' Organization Read more about NER on Wikipedia .","title":"Example Task:"},{"location":"#train-your-own-nerda-model","text":"Say, we want to fine-tune a pretrained Multilingual BERT transformer for NER in English. Load package. from NERDA.models import NERDA Instantiate a NERDA model ( with default settings ) for the CoNLL-2003 English NER data set. from NERDA.datasets import get_conll_data model = NERDA(dataset_training = get_conll_data('train'), dataset_validation = get_conll_data('valid'), transformer = 'bert-base-multilingual-uncased') By default the network architecture is analogous to that of the models in Hvingelby et al. 2020 . The model can then be trained/fine-tuned by invoking the train method, e.g. model.train() Note : this will take some time depending on the dimensions of your machine (if you want to skip training, you can go ahead and use one of the models, that we have already precooked for you in stead). After the model has been trained, the model can be used for predicting named entities in new texts. # text to identify named entities in. text = 'Old MacDonald had a farm' model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']]) This means, that the model identified 'Old MacDonald' as a PER son. Please note, that the NERDA model configuration above was instantiated with all default settings. You can however customize your NERDA model in a lot of ways: Use your own data set (finetune a transformer for any given language) Choose whatever transformer you like Set all of the hyperparameters for the model You can even apply your own Network Architecture Read more about advanced usage of NERDA in the detailed documentation .","title":"Train Your Own NERDA Model"},{"location":"#use-a-precooked-nerda-model","text":"We have precooked a number of NERDA models for Danish and English, that you can download and use right off the shelf. Here is an example. Instantiate a multilingual BERT model, that has been finetuned for NER in Danish, DA_BERT_ML . from NERDA.precooked import DA_BERT_ML() model = DA_BERT_ML() Down(load) network from web: model.download_network() model.load_network() You can now predict named entities in new (Danish) texts # (Danish) text to identify named entities in: # 'Jens Hansen har en bondeg\u00e5rd' = 'Old MacDonald had a farm' text = 'Jens Hansen har en bondeg\u00e5rd' model.predict_text(text) ([['Jens', 'Hansen', 'har', 'en', 'bondeg\u00e5rd']], [['B-PER', 'I-PER', 'O', 'O', 'O']])","title":"Use a Precooked NERDA model"},{"location":"#list-of-precooked-models","text":"The table below shows the precooked NERDA models publicly available for download. Model Language Transformer Dataset F1-score DA_BERT_ML Danish Multilingual BERT DaNE 82.8 DA_ELECTRA_DA Danish Danish ELECTRA DaNE 79.8 EN_BERT_ML English Multilingual BERT CoNLL-2003 90.4 EN_ELECTRA_EN English English ELECTRA CoNLL-2003 89.1 F1-score is the micro-averaged F1-score across entity tags and is evaluated on the respective test sets (that have not been used for training nor validation of the models). Note, that we have not spent a lot of time on actually fine-tuning the models, so there could be room for improvement. If you are able to improve the models, we will be happy to hear from you and include your NERDA model.","title":"List of Precooked Models"},{"location":"#model-performance","text":"The table below summarizes the performance (F1-scores) of the precooked NERDA models. Level DA_BERT_ML DA_ELECTRA_DA EN_BERT_ML EN_ELECTRA_EN B-PER 93.8 92.0 96.0 95.1 I-PER 97.8 97.1 98.5 97.9 B-ORG 69.5 66.9 88.4 86.2 I-ORG 69.9 70.7 85.7 83.1 B-LOC 82.5 79.0 92.3 91.1 I-LOC 31.6 44.4 83.9 80.5 B-MISC 73.4 68.6 81.8 80.1 I-MISC 86.1 63.6 63.4 68.4 AVG_MICRO 82.8 79.8 90.4 89.1 AVG_MACRO 75.6 72.8 86.3 85.3","title":"Model Performance"},{"location":"#nerda_1","text":"' NERDA ' originally stands for 'Named Entity Recognition for DAnish' . However, this is somewhat misleading, since the functionality is no longer limited to Danish. On the contrary it generalizes to all other languages, i.e. NERDA supports fine-tuning of transformers for NER tasks for any arbitrary language.","title":"'NERDA'?"},{"location":"#background","text":"NERDA is developed as a part of Ekstra Bladet \u2019s activities on Platform Intelligence in News (PIN). PIN is an industrial research project that is carried out in collaboration between the Technical University of Denmark , University of Copenhagen and Copenhagen Business School with funding from Innovation Fund Denmark . The project runs from 2020-2023 and develops recommender systems and natural language processing systems geared for news publishing, some of which are open sourced like NERDA .","title":"Background"},{"location":"#shout-outs","text":"Thanks to Alexandra Institute for with the danlp package to have encouraged us to develop this package. Thanks to Malte H\u00f8jmark-Bertelsen and Kasper Junge for giving feedback on NERDA .","title":"Shout-outs"},{"location":"#contact","text":"We hope, that you will find NERDA useful. Please direct any questions and feedbacks to us ! If you want to contribute (which we encourage you to), open a PR . If you encounter a bug or want to suggest an enhancement, please open an issue .","title":"Contact"},{"location":"datasets/","text":"Datasets This section covers functionality for (down)loading Named Entity Recognition data sets. download_conll_data ( dir = None ) Download CoNLL-2003 English data set. Downloads the CoNLL-2003 English data set annotated for Named Entity Recognition. Parameters: Name Type Description Default dir str Directory where CoNLL-2003 datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. None Returns: Type Description str str: a message telling, if the archive was in fact succesfully extracted. Obviously the CoNLL datasets are extracted to the desired directory as a side-effect. Examples: >>> download_conll_data () >>> download_conll_data ( dir = 'conll' ) Source code in NERDA/datasets.py def download_conll_data ( dir : str = None ) -> str : \"\"\"Download CoNLL-2003 English data set. Downloads the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) English data set annotated for Named Entity Recognition. Args: dir (str, optional): Directory where CoNLL-2003 datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. Returns: str: a message telling, if the archive was in fact succesfully extracted. Obviously the CoNLL datasets are extracted to the desired directory as a side-effect. Examples: >>> download_conll_data() >>> download_conll_data(dir = 'conll') \"\"\" # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.conll' ) return download_unzip ( url_zip = 'https://data.deepai.org/conll2003.zip' , dir_extract = dir ) download_dane_data ( dir = None ) Download DaNE data set. Downloads the 'DaNE' data set annotated for Named Entity Recognition developed and hosted by Alexandra Institute . Parameters: Name Type Description Default dir str Directory where DaNE datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. None Returns: Type Description str str: a message telling, if the archive was in fact succesfully extracted. Obviously the DaNE datasets are extracted to the desired directory as a side-effect. Examples: >>> download_dane_data () >>> download_dane_data ( dir = 'DaNE' ) Source code in NERDA/datasets.py def download_dane_data ( dir : str = None ) -> str : \"\"\"Download DaNE data set. Downloads the 'DaNE' data set annotated for Named Entity Recognition developed and hosted by [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane). Args: dir (str, optional): Directory where DaNE datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. Returns: str: a message telling, if the archive was in fact succesfully extracted. Obviously the DaNE datasets are extracted to the desired directory as a side-effect. Examples: >>> download_dane_data() >>> download_dane_data(dir = 'DaNE') \"\"\" # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.dane' ) return download_unzip ( url_zip = 'http://danlp-downloads.alexandra.dk/datasets/ddt.zip' , dir_extract = dir ) download_unzip ( url_zip , dir_extract ) Download and unzip a ZIP archive to folder. Loads a ZIP file from URL and extracts all of the files to a given folder. Does not save the ZIP file itself. Parameters: Name Type Description Default url_zip str URL to ZIP file. required dir_extract str Directory where files are extracted. required Returns: Type Description str str: a message telling, if the archive was succesfully extracted. Obviously the files in the ZIP archive are extracted to the desired directory as a side-effect. Source code in NERDA/datasets.py def download_unzip ( url_zip : str , dir_extract : str ) -> str : \"\"\"Download and unzip a ZIP archive to folder. Loads a ZIP file from URL and extracts all of the files to a given folder. Does not save the ZIP file itself. Args: url_zip (str): URL to ZIP file. dir_extract (str): Directory where files are extracted. Returns: str: a message telling, if the archive was succesfully extracted. Obviously the files in the ZIP archive are extracted to the desired directory as a side-effect. \"\"\" # suppress ssl certification ctx = ssl . create_default_context () ctx . check_hostname = False ctx . verify_mode = ssl . CERT_NONE print ( f 'Reading { url_zip } ' ) with urlopen ( url_zip , context = ctx ) as zipresp : with ZipFile ( BytesIO ( zipresp . read ())) as zfile : zfile . extractall ( dir_extract ) return f 'archive extracted to { dir_extract } ' get_conll_data ( split = 'train' , limit = None , dir = None ) Load CoNLL-2003 (English) data split. Loads a single data split from the CoNLL-2003 (English) data set. Parameters: Name Type Description Default split str Choose which split to load. Choose from 'train', 'valid' and 'test'. Defaults to 'train'. 'train' limit int Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. None dir str Directory where data is cached. If set to None, the function will try to look for files in '.conll' folder in home directory. None Returns: Type Description dict dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_conll_data ( 'test' ) Get first 5 observations from training split >>> get_conll_data ( 'train' , limit = 5 ) Source code in NERDA/datasets.py def get_conll_data ( split : str = 'train' , limit : int = None , dir : str = None ) -> dict : \"\"\"Load CoNLL-2003 (English) data split. Loads a single data split from the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) (English) data set. Args: split (str, optional): Choose which split to load. Choose from 'train', 'valid' and 'test'. Defaults to 'train'. limit (int, optional): Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. dir (str, optional): Directory where data is cached. If set to None, the function will try to look for files in '.conll' folder in home directory. Returns: dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_conll_data('test') Get first 5 observations from training split >>> get_conll_data('train', limit = 5) \"\"\" assert isinstance ( split , str ) splits = [ 'train' , 'valid' , 'test' ] assert split in splits , f 'Choose between the following splits: { splits } ' # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.conll' ) assert os . path . isdir ( dir ), f 'Directory { dir } does not exist. Try downloading CoNLL-2003 data with download_conll_data()' file_path = os . path . join ( dir , f ' { split } .txt' ) assert os . path . isfile ( file_path ), f 'File { file_path } does not exist. Try downloading CoNLL-2003 data with download_conll_data()' # read data from file. data = [] with open ( file_path , 'r' ) as file : reader = csv . reader ( file , delimiter = ' ' ) for row in reader : data . append ([ row ]) sentences = [] sentence = [] entities = [] tags = [] for row in data : # extract first element of list. row = row [ 0 ] # TO DO: move to data reader. if len ( row ) > 0 and row [ 0 ] != '-DOCSTART-' : sentence . append ( row [ 0 ]) tags . append ( row [ - 1 ]) if len ( row ) == 0 and len ( sentence ) > 0 : # clean up sentence/tags. # remove white spaces. selector = [ word != ' ' for word in sentence ] sentence = list ( compress ( sentence , selector )) tags = list ( compress ( tags , selector )) # append if sentence length is still greater than zero.. if len ( sentence ) > 0 : sentences . append ( sentence ) entities . append ( tags ) sentence = [] tags = [] if limit is not None : sentences = sentences [: limit ] entities = entities [: limit ] return { 'sentences' : sentences , 'tags' : entities } get_dane_data ( split = 'train' , limit = None , dir = None ) Load DaNE data split. Loads a single data split from the DaNE data set kindly hosted by Alexandra Institute . Parameters: Name Type Description Default split str Choose which split to load. Choose from 'train', 'dev' and 'test'. Defaults to 'train'. 'train' limit int Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. None dir str Directory where data is cached. If set to None, the function will try to look for files in '.dane' folder in home directory. None Returns: Type Description dict dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_dane_data ( 'test' ) Get first 5 observations from training split >>> get_dane_data ( 'train' , limit = 5 ) Source code in NERDA/datasets.py def get_dane_data ( split : str = 'train' , limit : int = None , dir : str = None ) -> dict : \"\"\"Load DaNE data split. Loads a single data split from the DaNE data set kindly hosted by [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane). Args: split (str, optional): Choose which split to load. Choose from 'train', 'dev' and 'test'. Defaults to 'train'. limit (int, optional): Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. dir (str, optional): Directory where data is cached. If set to None, the function will try to look for files in '.dane' folder in home directory. Returns: dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_dane_data('test') Get first 5 observations from training split >>> get_dane_data('train', limit = 5) \"\"\" assert isinstance ( split , str ) splits = [ 'train' , 'dev' , 'test' ] assert split in splits , f 'Choose between the following splits: { splits } ' # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.dane' ) assert os . path . isdir ( dir ), f 'Directory { dir } does not exist. Try downloading DaNE data with download_dane_data()' file_path = os . path . join ( dir , f 'ddt. { split } .conllu' ) assert os . path . isfile ( file_path ), f 'File { file_path } does not exist. Try downloading DaNE data with download_dane_data()' split = pyconll . load_from_file ( file_path ) sentences = [] entities = [] for sent in split : sentences . append ([ token . form for token in sent . _tokens ]) entities . append ([ token . misc [ 'name' ] . pop () for token in sent . _tokens ]) if limit is not None : sentences = sentences [: limit ] entities = entities [: limit ] return { 'sentences' : sentences , 'tags' : entities }","title":"Datasets"},{"location":"datasets/#datasets","text":"This section covers functionality for (down)loading Named Entity Recognition data sets.","title":"Datasets"},{"location":"datasets/#NERDA.datasets.download_conll_data","text":"Download CoNLL-2003 English data set. Downloads the CoNLL-2003 English data set annotated for Named Entity Recognition. Parameters: Name Type Description Default dir str Directory where CoNLL-2003 datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. None Returns: Type Description str str: a message telling, if the archive was in fact succesfully extracted. Obviously the CoNLL datasets are extracted to the desired directory as a side-effect. Examples: >>> download_conll_data () >>> download_conll_data ( dir = 'conll' ) Source code in NERDA/datasets.py def download_conll_data ( dir : str = None ) -> str : \"\"\"Download CoNLL-2003 English data set. Downloads the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) English data set annotated for Named Entity Recognition. Args: dir (str, optional): Directory where CoNLL-2003 datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. Returns: str: a message telling, if the archive was in fact succesfully extracted. Obviously the CoNLL datasets are extracted to the desired directory as a side-effect. Examples: >>> download_conll_data() >>> download_conll_data(dir = 'conll') \"\"\" # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.conll' ) return download_unzip ( url_zip = 'https://data.deepai.org/conll2003.zip' , dir_extract = dir )","title":"download_conll_data()"},{"location":"datasets/#NERDA.datasets.download_dane_data","text":"Download DaNE data set. Downloads the 'DaNE' data set annotated for Named Entity Recognition developed and hosted by Alexandra Institute . Parameters: Name Type Description Default dir str Directory where DaNE datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. None Returns: Type Description str str: a message telling, if the archive was in fact succesfully extracted. Obviously the DaNE datasets are extracted to the desired directory as a side-effect. Examples: >>> download_dane_data () >>> download_dane_data ( dir = 'DaNE' ) Source code in NERDA/datasets.py def download_dane_data ( dir : str = None ) -> str : \"\"\"Download DaNE data set. Downloads the 'DaNE' data set annotated for Named Entity Recognition developed and hosted by [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane). Args: dir (str, optional): Directory where DaNE datasets will be saved. If no directory is provided, data will be saved to a hidden folder '.dane' in your home directory. Returns: str: a message telling, if the archive was in fact succesfully extracted. Obviously the DaNE datasets are extracted to the desired directory as a side-effect. Examples: >>> download_dane_data() >>> download_dane_data(dir = 'DaNE') \"\"\" # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.dane' ) return download_unzip ( url_zip = 'http://danlp-downloads.alexandra.dk/datasets/ddt.zip' , dir_extract = dir )","title":"download_dane_data()"},{"location":"datasets/#NERDA.datasets.download_unzip","text":"Download and unzip a ZIP archive to folder. Loads a ZIP file from URL and extracts all of the files to a given folder. Does not save the ZIP file itself. Parameters: Name Type Description Default url_zip str URL to ZIP file. required dir_extract str Directory where files are extracted. required Returns: Type Description str str: a message telling, if the archive was succesfully extracted. Obviously the files in the ZIP archive are extracted to the desired directory as a side-effect. Source code in NERDA/datasets.py def download_unzip ( url_zip : str , dir_extract : str ) -> str : \"\"\"Download and unzip a ZIP archive to folder. Loads a ZIP file from URL and extracts all of the files to a given folder. Does not save the ZIP file itself. Args: url_zip (str): URL to ZIP file. dir_extract (str): Directory where files are extracted. Returns: str: a message telling, if the archive was succesfully extracted. Obviously the files in the ZIP archive are extracted to the desired directory as a side-effect. \"\"\" # suppress ssl certification ctx = ssl . create_default_context () ctx . check_hostname = False ctx . verify_mode = ssl . CERT_NONE print ( f 'Reading { url_zip } ' ) with urlopen ( url_zip , context = ctx ) as zipresp : with ZipFile ( BytesIO ( zipresp . read ())) as zfile : zfile . extractall ( dir_extract ) return f 'archive extracted to { dir_extract } '","title":"download_unzip()"},{"location":"datasets/#NERDA.datasets.get_conll_data","text":"Load CoNLL-2003 (English) data split. Loads a single data split from the CoNLL-2003 (English) data set. Parameters: Name Type Description Default split str Choose which split to load. Choose from 'train', 'valid' and 'test'. Defaults to 'train'. 'train' limit int Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. None dir str Directory where data is cached. If set to None, the function will try to look for files in '.conll' folder in home directory. None Returns: Type Description dict dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_conll_data ( 'test' ) Get first 5 observations from training split >>> get_conll_data ( 'train' , limit = 5 ) Source code in NERDA/datasets.py def get_conll_data ( split : str = 'train' , limit : int = None , dir : str = None ) -> dict : \"\"\"Load CoNLL-2003 (English) data split. Loads a single data split from the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) (English) data set. Args: split (str, optional): Choose which split to load. Choose from 'train', 'valid' and 'test'. Defaults to 'train'. limit (int, optional): Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. dir (str, optional): Directory where data is cached. If set to None, the function will try to look for files in '.conll' folder in home directory. Returns: dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_conll_data('test') Get first 5 observations from training split >>> get_conll_data('train', limit = 5) \"\"\" assert isinstance ( split , str ) splits = [ 'train' , 'valid' , 'test' ] assert split in splits , f 'Choose between the following splits: { splits } ' # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.conll' ) assert os . path . isdir ( dir ), f 'Directory { dir } does not exist. Try downloading CoNLL-2003 data with download_conll_data()' file_path = os . path . join ( dir , f ' { split } .txt' ) assert os . path . isfile ( file_path ), f 'File { file_path } does not exist. Try downloading CoNLL-2003 data with download_conll_data()' # read data from file. data = [] with open ( file_path , 'r' ) as file : reader = csv . reader ( file , delimiter = ' ' ) for row in reader : data . append ([ row ]) sentences = [] sentence = [] entities = [] tags = [] for row in data : # extract first element of list. row = row [ 0 ] # TO DO: move to data reader. if len ( row ) > 0 and row [ 0 ] != '-DOCSTART-' : sentence . append ( row [ 0 ]) tags . append ( row [ - 1 ]) if len ( row ) == 0 and len ( sentence ) > 0 : # clean up sentence/tags. # remove white spaces. selector = [ word != ' ' for word in sentence ] sentence = list ( compress ( sentence , selector )) tags = list ( compress ( tags , selector )) # append if sentence length is still greater than zero.. if len ( sentence ) > 0 : sentences . append ( sentence ) entities . append ( tags ) sentence = [] tags = [] if limit is not None : sentences = sentences [: limit ] entities = entities [: limit ] return { 'sentences' : sentences , 'tags' : entities }","title":"get_conll_data()"},{"location":"datasets/#NERDA.datasets.get_dane_data","text":"Load DaNE data split. Loads a single data split from the DaNE data set kindly hosted by Alexandra Institute . Parameters: Name Type Description Default split str Choose which split to load. Choose from 'train', 'dev' and 'test'. Defaults to 'train'. 'train' limit int Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. None dir str Directory where data is cached. If set to None, the function will try to look for files in '.dane' folder in home directory. None Returns: Type Description dict dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_dane_data ( 'test' ) Get first 5 observations from training split >>> get_dane_data ( 'train' , limit = 5 ) Source code in NERDA/datasets.py def get_dane_data ( split : str = 'train' , limit : int = None , dir : str = None ) -> dict : \"\"\"Load DaNE data split. Loads a single data split from the DaNE data set kindly hosted by [Alexandra Institute](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane). Args: split (str, optional): Choose which split to load. Choose from 'train', 'dev' and 'test'. Defaults to 'train'. limit (int, optional): Limit the number of observations to be returned from a given split. Defaults to None, which implies that the entire data split is returned. dir (str, optional): Directory where data is cached. If set to None, the function will try to look for files in '.dane' folder in home directory. Returns: dict: Dictionary with word-tokenized 'sentences' and named entity 'tags' in IOB format. Examples: Get test split >>> get_dane_data('test') Get first 5 observations from training split >>> get_dane_data('train', limit = 5) \"\"\" assert isinstance ( split , str ) splits = [ 'train' , 'dev' , 'test' ] assert split in splits , f 'Choose between the following splits: { splits } ' # set to default directory if nothing else has been provided by user. if dir is None : dir = os . path . join ( str ( Path . home ()), '.dane' ) assert os . path . isdir ( dir ), f 'Directory { dir } does not exist. Try downloading DaNE data with download_dane_data()' file_path = os . path . join ( dir , f 'ddt. { split } .conllu' ) assert os . path . isfile ( file_path ), f 'File { file_path } does not exist. Try downloading DaNE data with download_dane_data()' split = pyconll . load_from_file ( file_path ) sentences = [] entities = [] for sent in split : sentences . append ([ token . form for token in sent . _tokens ]) entities . append ([ token . misc [ 'name' ] . pop () for token in sent . _tokens ]) if limit is not None : sentences = sentences [: limit ] entities = entities [: limit ] return { 'sentences' : sentences , 'tags' : entities }","title":"get_dane_data()"},{"location":"nerda_models/","text":"NERDA Models This section covers the interface for NERDA models, that is implemented as its own Python class NERDA.models.NERDA . The interface enables you to easily specify your own NERDA.models.NERDA model train it evaluate it use it to predict entities in new texts. NERDA NERDA model A NERDA model object containing a complete model configuration. The model can be trained with the train method. Afterwards new observations can be predicted with the predict and predict_text methods. The performance of the model can be evaluated on a set of new observations with the evaluate_performance method. Examples: Model for a VERY small subset (5 observations) of English NER data >>> from NERDA.datasets import get_conll_data >>> trn = get_conll_data ( 'train' , 5 ) >>> valid = get_conll_data ( 'valid' , 5 ) >>> tag_scheme = [ 'B-PER' , 'I-PER' , 'B-LOC' , 'I-LOC' , 'B-ORG' , 'I-ORG' , 'B-MISC' , 'I-MISC' ] >>> tag_outside = 'O' >>> transformer = 'bert-base-multilingual-uncased' >>> model = NERDA ( transformer = transformer , tag_scheme = tag_scheme , tag_outside = tag_outside , dataset_training = trn , dataset_validation = valid ) Model for complete English NER data set CoNLL-2003 with modified hyperparameters >>> trn = get_conll_data ( 'train' ) >>> valid = get_conll_data ( 'valid' ) >>> transformer = 'bert-base-multilingual-uncased' >>> hyperparameters = { 'epochs' : 3 , 'warmup_steps' : 400 , 'train_batch_size' : 16 , 'learning_rate' : 0.0001 }, >>> model = NERDA ( transformer = transformer , dataset_training = trn , dataset_validation = valid , tag_scheme = tag_scheme , tag_outside = tag_outside , dropout = 0.1 , hyperparameters = hyperparameters ) Attributes: Name Type Description network torch.nn.Module network for Named Entity Recognition task. tag_encoder sklearn.preprocessing.LabelEncoder encoder for the NER labels/tags. transformer_model transformers.PreTrainedModel (Auto)Model derived from the transformer. transformer_tokenizer transformers.PretrainedTokenizer (Auto)Tokenizer derived from the transformer. transformer_config transformers.PretrainedConfig (Auto)Config derived from the transformer. train_losses list holds training losses, once the model has been trained. valid_loss float holds validation loss, once the model has been trained. __init__ ( self , transformer = 'bert-base-multilingual-uncased' , device = None , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , dataset_training = None , dataset_validation = None , max_len = 128 , network =< class ' NERDA . networks . NERDANetwork '>, dropout=0.1, hyperparameters={' epochs ': 4, ' warmup_steps ': 500, ' train_batch_size ': 13, ' learning_rate ': 0.0001}, tokenizer_parameters={' do_lower_case ': True}, validation_batch_size=8, num_workers=1) special Initialize NERDA model Parameters: Name Type Description Default transformer str which pretrained 'huggingface' transformer to use. 'bert-base-multilingual-uncased' device str the desired device to use for computation. If not provided by the user, we take a guess. None tag_scheme List[str] All available NER tags for the given data set EXCLUDING the special outside tag, that is handled separately. ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'] tag_outside str the value of the special outside tag. Defaults to 'O'. 'O' dataset_training dict the training data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. None dataset_validation dict the validation data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. None max_len int the maximum sentence length (number of tokens after applying the transformer tokenizer) for the transformer. Sentences are truncated accordingly. Look at your data to get an impression of, what could be a meaningful setting. Also be aware that many transformers have a maximum accepted length. Defaults to 128. 128 network Module network to be trained. Defaults to a default generic NERDANetwork . Can be replaced with your own customized network architecture. It must however take the same arguments as NERDANetwork . <class 'NERDA.networks.NERDANetwork'> dropout float dropout probability. Defaults to 0.1. 0.1 hyperparameters dict Hyperparameters for the model. Defaults to {'epochs' : 3, 'warmup_steps' : 500, 'train_batch_size': 16, 'learning_rate': 0.0001}. {'epochs': 4, 'warmup_steps': 500, 'train_batch_size': 13, 'learning_rate': 0.0001} tokenizer_parameters dict parameters for the transformer tokenizer. Defaults to {'do_lower_case' : True}. {'do_lower_case': True} validation_batch_size int batch size for validation. Defaults to 8. 8 num_workers int number of workers for data loader. 1 Source code in NERDA/models.py def __init__ ( self , transformer : str = 'bert-base-multilingual-uncased' , device : str = None , tag_scheme : List [ str ] = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside : str = 'O' , dataset_training : dict = None , dataset_validation : dict = None , max_len : int = 128 , network : torch . nn . Module = NERDANetwork , dropout : float = 0.1 , hyperparameters : dict = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters : dict = { 'do_lower_case' : True }, validation_batch_size : int = 8 , num_workers : int = 1 ) -> None : \"\"\"Initialize NERDA model Args: transformer (str, optional): which pretrained 'huggingface' transformer to use. device (str, optional): the desired device to use for computation. If not provided by the user, we take a guess. tag_scheme (List[str], optional): All available NER tags for the given data set EXCLUDING the special outside tag, that is handled separately. tag_outside (str, optional): the value of the special outside tag. Defaults to 'O'. dataset_training (dict, optional): the training data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. dataset_validation (dict, optional): the validation data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. max_len (int, optional): the maximum sentence length (number of tokens after applying the transformer tokenizer) for the transformer. Sentences are truncated accordingly. Look at your data to get an impression of, what could be a meaningful setting. Also be aware that many transformers have a maximum accepted length. Defaults to 128. network (torch.nn.module, optional): network to be trained. Defaults to a default generic `NERDANetwork`. Can be replaced with your own customized network architecture. It must however take the same arguments as `NERDANetwork`. dropout (float, optional): dropout probability. Defaults to 0.1. hyperparameters (dict, optional): Hyperparameters for the model. Defaults to {'epochs' : 3, 'warmup_steps' : 500, 'train_batch_size': 16, 'learning_rate': 0.0001}. tokenizer_parameters (dict, optional): parameters for the transformer tokenizer. Defaults to {'do_lower_case' : True}. validation_batch_size (int, optional): batch size for validation. Defaults to 8. num_workers (int, optional): number of workers for data loader. \"\"\" # set device automatically if not provided by user. if device is None : self . device = 'cuda' if torch . cuda . is_available () else 'cpu' print ( \"Device automatically set to:\" , self . device ) else : self . device = device print ( \"Device set to:\" , self . device ) self . tag_scheme = tag_scheme self . tag_outside = tag_outside self . transformer = transformer self . dataset_training = dataset_training self . dataset_validation = dataset_validation self . hyperparameters = hyperparameters self . tag_outside = tag_outside self . tag_scheme = tag_scheme tag_complete = [ tag_outside ] + tag_scheme # fit encoder to _all_ possible tags. self . max_len = max_len self . tag_encoder = sklearn . preprocessing . LabelEncoder () self . tag_encoder . fit ( tag_complete ) self . transformer_model = AutoModel . from_pretrained ( transformer ) self . transformer_tokenizer = AutoTokenizer . from_pretrained ( transformer , ** tokenizer_parameters ) self . transformer_config = AutoConfig . from_pretrained ( transformer ) self . network = NERDANetwork ( self . transformer_model , self . device , len ( tag_complete ), dropout = dropout ) self . network . to ( self . device ) self . validation_batch_size = validation_batch_size self . num_workers = num_workers self . train_losses = [] self . valid_loss = np . nan self . quantized = False self . halved = False evaluate_performance ( self , dataset , return_accuracy = False , ** kwargs ) Evaluate Performance Evaluates the performance of the model on an arbitrary data set. Parameters: Name Type Description Default dataset dict Data set that must consist of 'sentences' and NER'tags'. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). required kwargs arbitrary keyword arguments for predict. For instance 'batch_size' and 'num_workers'. {} return_accuracy bool Return accuracy as well? Defaults to False. False Returns: Type Description DataFrame DataFrame with performance numbers, F1-scores, Precision and Recall. Returns dictionary with this AND accuracy, if return_accuracy is set to True. Source code in NERDA/models.py def evaluate_performance ( self , dataset : dict , return_accuracy : bool = False , ** kwargs ) -> pd . DataFrame : \"\"\"Evaluate Performance Evaluates the performance of the model on an arbitrary data set. Args: dataset (dict): Data set that must consist of 'sentences' and NER'tags'. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). kwargs: arbitrary keyword arguments for predict. For instance 'batch_size' and 'num_workers'. return_accuracy (bool): Return accuracy as well? Defaults to False. Returns: DataFrame with performance numbers, F1-scores, Precision and Recall. Returns dictionary with this AND accuracy, if return_accuracy is set to True. \"\"\" tags_predicted = self . predict ( dataset . get ( 'sentences' ), ** kwargs ) # compute F1 scores by entity type f1 = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = None ) # create DataFrame with performance scores (=F1) df = list ( zip ( self . tag_scheme , f1 [ 2 ], f1 [ 0 ], f1 [ 1 ])) df = pd . DataFrame ( df , columns = [ 'Level' , 'F1-Score' , 'Precision' , 'Recall' ]) # compute MICRO-averaged F1-scores and add to table. f1_micro = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = 'micro' ) f1_micro = pd . DataFrame ({ 'Level' : [ 'AVG_MICRO' ], 'F1-Score' : [ f1_micro [ 2 ]], 'Precision' : [ np . nan ], 'Recall' : [ np . nan ]}) df = df . append ( f1_micro ) # compute MACRO-averaged F1-scores and add to table. f1_macro = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = 'macro' ) f1_macro = pd . DataFrame ({ 'Level' : [ 'AVG_MICRO' ], 'F1-Score' : [ f1_macro [ 2 ]], 'Precision' : [ np . nan ], 'Recall' : [ np . nan ]}) df = df . append ( f1_macro ) # compute and return accuracy if desired if return_accuracy : accuracy = accuracy_score ( y_pred = flatten ( tags_predicted ), y_true = flatten ( dataset . get ( 'tags' ))) return { 'f1' : df , 'accuracy' : accuracy } return df half ( self ) Convert weights from Float32 to Float16 to increase performance Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=half#torch.nn.Module.half Returns: Nothing. Model is \"halved\" as a side-effect. Source code in NERDA/models.py def half ( self ): \"\"\"Convert weights from Float32 to Float16 to increase performance Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=half#torch.nn.Module.half Returns: Nothing. Model is \"halved\" as a side-effect. \"\"\" assert not ( self . halved ), \"Half precision already applied\" assert not ( self . quantized ), \"Can't run both quantization and half precision\" self . network . half () self . halved = True load_network_from_file ( self , model_path = 'model.bin' ) Load Pretrained NERDA Network from file Loads weights for a pretrained NERDA Network from file. Parameters: Name Type Description Default model_path str Path for model file. Defaults to \"model.bin\". 'model.bin' Returns: Type Description str str: message telling if weights for network were loaded succesfully. Source code in NERDA/models.py def load_network_from_file ( self , model_path = \"model.bin\" ) -> str : \"\"\"Load Pretrained NERDA Network from file Loads weights for a pretrained NERDA Network from file. Args: model_path (str, optional): Path for model file. Defaults to \"model.bin\". Returns: str: message telling if weights for network were loaded succesfully. \"\"\" # TODO: change assert to Raise. assert os . path . exists ( model_path ), \"File does not exist. You can download network with download_network()\" self . network . load_state_dict ( torch . load ( model_path , map_location = torch . device ( self . device ))) self . network . device = self . device return f 'Weights for network loaded from { model_path } ' predict ( self , sentences , return_confidence = False , ** kwargs ) Predict Named Entities in Word-Tokenized Sentences Predicts word-tokenized sentences with trained model. Parameters: Name Type Description Default sentences List[List[str]] word-tokenized sentences. required kwargs arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. {} return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False Returns: Type Description List[List[str]] List[List[str]]: Predicted tags for sentences - one predicted tag/entity per word token. Source code in NERDA/models.py def predict ( self , sentences : List [ List [ str ]], return_confidence : bool = False , ** kwargs ) -> List [ List [ str ]]: \"\"\"Predict Named Entities in Word-Tokenized Sentences Predicts word-tokenized sentences with trained model. Args: sentences (List[List[str]]): word-tokenized sentences. kwargs: arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. Returns: List[List[str]]: Predicted tags for sentences - one predicted tag/entity per word token. \"\"\" return predict ( network = self . network , sentences = sentences , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , max_len = self . max_len , device = self . device , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , return_confidence = return_confidence , ** kwargs ) predict_text ( self , text , return_confidence = False , ** kwargs ) Predict Named Entities in a Text Parameters: Name Type Description Default text str text to predict entities in. required kwargs arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. {} return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False Returns: Type Description list tuple: word-tokenized sentences and predicted tags/entities. Source code in NERDA/models.py def predict_text ( self , text : str , return_confidence : bool = False , ** kwargs ) -> list : \"\"\"Predict Named Entities in a Text Args: text (str): text to predict entities in. kwargs: arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. Returns: tuple: word-tokenized sentences and predicted tags/entities. \"\"\" return predict_text ( network = self . network , text = text , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , max_len = self . max_len , device = self . device , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , return_confidence = return_confidence , ** kwargs ) quantize ( self ) Apply dynamic quantization to increase performance. Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html Returns: Type Description Nothing. Applies dynamic quantization to Network as a side-effect. Source code in NERDA/models.py def quantize ( self ): \"\"\"Apply dynamic quantization to increase performance. Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html Returns: Nothing. Applies dynamic quantization to Network as a side-effect. \"\"\" assert not ( self . quantized ), \"Dynamic quantization already applied\" assert not ( self . halved ), \"Can't run both quantization and half precision\" self . network = torch . quantization . quantize_dynamic ( self . network , { torch . nn . Linear }, dtype = torch . qint8 ) self . quantized = True save_network ( self , model_path = 'model.bin' ) Save Weights of NERDA Network Saves weights for a fine-tuned NERDA Network to file. Parameters: Name Type Description Default model_path str Path for model file. Defaults to \"model.bin\". 'model.bin' Returns: Type Description None Nothing. Saves model to file as a side-effect. Source code in NERDA/models.py def save_network ( self , model_path : str = \"model.bin\" ) -> None : \"\"\"Save Weights of NERDA Network Saves weights for a fine-tuned NERDA Network to file. Args: model_path (str, optional): Path for model file. Defaults to \"model.bin\". Returns: Nothing. Saves model to file as a side-effect. \"\"\" torch . save ( self . network . state_dict (), model_path ) print ( f \"Network written to file { model_path } \" ) train ( self ) Train Network Trains the network from the NERDA model specification. Returns: Type Description str str: a message saying if the model was trained succesfully. The network in the 'network' attribute is trained as a side-effect. Training losses and validation loss are saved in 'training_losses' and 'valid_loss' attributes respectively as side-effects. Source code in NERDA/models.py def train ( self ) -> str : \"\"\"Train Network Trains the network from the NERDA model specification. Returns: str: a message saying if the model was trained succesfully. The network in the 'network' attribute is trained as a side-effect. Training losses and validation loss are saved in 'training_losses' and 'valid_loss' attributes respectively as side-effects. \"\"\" network , train_losses , valid_loss = train_model ( network = self . network , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , dataset_training = self . dataset_training , dataset_validation = self . dataset_validation , validation_batch_size = self . validation_batch_size , max_len = self . max_len , device = self . device , num_workers = self . num_workers , ** self . hyperparameters ) # attach as attributes to class setattr ( self , \"network\" , network ) setattr ( self , \"train_losses\" , train_losses ) setattr ( self , \"valid_loss\" , valid_loss ) return \"Model trained successfully\"","title":"NERDA Models"},{"location":"nerda_models/#nerda-models","text":"This section covers the interface for NERDA models, that is implemented as its own Python class NERDA.models.NERDA . The interface enables you to easily specify your own NERDA.models.NERDA model train it evaluate it use it to predict entities in new texts.","title":"NERDA Models"},{"location":"nerda_models/#NERDA.models.NERDA","text":"NERDA model A NERDA model object containing a complete model configuration. The model can be trained with the train method. Afterwards new observations can be predicted with the predict and predict_text methods. The performance of the model can be evaluated on a set of new observations with the evaluate_performance method. Examples: Model for a VERY small subset (5 observations) of English NER data >>> from NERDA.datasets import get_conll_data >>> trn = get_conll_data ( 'train' , 5 ) >>> valid = get_conll_data ( 'valid' , 5 ) >>> tag_scheme = [ 'B-PER' , 'I-PER' , 'B-LOC' , 'I-LOC' , 'B-ORG' , 'I-ORG' , 'B-MISC' , 'I-MISC' ] >>> tag_outside = 'O' >>> transformer = 'bert-base-multilingual-uncased' >>> model = NERDA ( transformer = transformer , tag_scheme = tag_scheme , tag_outside = tag_outside , dataset_training = trn , dataset_validation = valid ) Model for complete English NER data set CoNLL-2003 with modified hyperparameters >>> trn = get_conll_data ( 'train' ) >>> valid = get_conll_data ( 'valid' ) >>> transformer = 'bert-base-multilingual-uncased' >>> hyperparameters = { 'epochs' : 3 , 'warmup_steps' : 400 , 'train_batch_size' : 16 , 'learning_rate' : 0.0001 }, >>> model = NERDA ( transformer = transformer , dataset_training = trn , dataset_validation = valid , tag_scheme = tag_scheme , tag_outside = tag_outside , dropout = 0.1 , hyperparameters = hyperparameters ) Attributes: Name Type Description network torch.nn.Module network for Named Entity Recognition task. tag_encoder sklearn.preprocessing.LabelEncoder encoder for the NER labels/tags. transformer_model transformers.PreTrainedModel (Auto)Model derived from the transformer. transformer_tokenizer transformers.PretrainedTokenizer (Auto)Tokenizer derived from the transformer. transformer_config transformers.PretrainedConfig (Auto)Config derived from the transformer. train_losses list holds training losses, once the model has been trained. valid_loss float holds validation loss, once the model has been trained.","title":"NERDA"},{"location":"nerda_models/#NERDA.models.NERDA.__init__","text":"Initialize NERDA model Parameters: Name Type Description Default transformer str which pretrained 'huggingface' transformer to use. 'bert-base-multilingual-uncased' device str the desired device to use for computation. If not provided by the user, we take a guess. None tag_scheme List[str] All available NER tags for the given data set EXCLUDING the special outside tag, that is handled separately. ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'] tag_outside str the value of the special outside tag. Defaults to 'O'. 'O' dataset_training dict the training data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. None dataset_validation dict the validation data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. None max_len int the maximum sentence length (number of tokens after applying the transformer tokenizer) for the transformer. Sentences are truncated accordingly. Look at your data to get an impression of, what could be a meaningful setting. Also be aware that many transformers have a maximum accepted length. Defaults to 128. 128 network Module network to be trained. Defaults to a default generic NERDANetwork . Can be replaced with your own customized network architecture. It must however take the same arguments as NERDANetwork . <class 'NERDA.networks.NERDANetwork'> dropout float dropout probability. Defaults to 0.1. 0.1 hyperparameters dict Hyperparameters for the model. Defaults to {'epochs' : 3, 'warmup_steps' : 500, 'train_batch_size': 16, 'learning_rate': 0.0001}. {'epochs': 4, 'warmup_steps': 500, 'train_batch_size': 13, 'learning_rate': 0.0001} tokenizer_parameters dict parameters for the transformer tokenizer. Defaults to {'do_lower_case' : True}. {'do_lower_case': True} validation_batch_size int batch size for validation. Defaults to 8. 8 num_workers int number of workers for data loader. 1 Source code in NERDA/models.py def __init__ ( self , transformer : str = 'bert-base-multilingual-uncased' , device : str = None , tag_scheme : List [ str ] = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside : str = 'O' , dataset_training : dict = None , dataset_validation : dict = None , max_len : int = 128 , network : torch . nn . Module = NERDANetwork , dropout : float = 0.1 , hyperparameters : dict = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters : dict = { 'do_lower_case' : True }, validation_batch_size : int = 8 , num_workers : int = 1 ) -> None : \"\"\"Initialize NERDA model Args: transformer (str, optional): which pretrained 'huggingface' transformer to use. device (str, optional): the desired device to use for computation. If not provided by the user, we take a guess. tag_scheme (List[str], optional): All available NER tags for the given data set EXCLUDING the special outside tag, that is handled separately. tag_outside (str, optional): the value of the special outside tag. Defaults to 'O'. dataset_training (dict, optional): the training data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. dataset_validation (dict, optional): the validation data. Must consist of 'sentences': word-tokenized sentences and 'tags': corresponding NER tags. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). Defaults to None, in which case the English CoNLL-2003 data set is used. max_len (int, optional): the maximum sentence length (number of tokens after applying the transformer tokenizer) for the transformer. Sentences are truncated accordingly. Look at your data to get an impression of, what could be a meaningful setting. Also be aware that many transformers have a maximum accepted length. Defaults to 128. network (torch.nn.module, optional): network to be trained. Defaults to a default generic `NERDANetwork`. Can be replaced with your own customized network architecture. It must however take the same arguments as `NERDANetwork`. dropout (float, optional): dropout probability. Defaults to 0.1. hyperparameters (dict, optional): Hyperparameters for the model. Defaults to {'epochs' : 3, 'warmup_steps' : 500, 'train_batch_size': 16, 'learning_rate': 0.0001}. tokenizer_parameters (dict, optional): parameters for the transformer tokenizer. Defaults to {'do_lower_case' : True}. validation_batch_size (int, optional): batch size for validation. Defaults to 8. num_workers (int, optional): number of workers for data loader. \"\"\" # set device automatically if not provided by user. if device is None : self . device = 'cuda' if torch . cuda . is_available () else 'cpu' print ( \"Device automatically set to:\" , self . device ) else : self . device = device print ( \"Device set to:\" , self . device ) self . tag_scheme = tag_scheme self . tag_outside = tag_outside self . transformer = transformer self . dataset_training = dataset_training self . dataset_validation = dataset_validation self . hyperparameters = hyperparameters self . tag_outside = tag_outside self . tag_scheme = tag_scheme tag_complete = [ tag_outside ] + tag_scheme # fit encoder to _all_ possible tags. self . max_len = max_len self . tag_encoder = sklearn . preprocessing . LabelEncoder () self . tag_encoder . fit ( tag_complete ) self . transformer_model = AutoModel . from_pretrained ( transformer ) self . transformer_tokenizer = AutoTokenizer . from_pretrained ( transformer , ** tokenizer_parameters ) self . transformer_config = AutoConfig . from_pretrained ( transformer ) self . network = NERDANetwork ( self . transformer_model , self . device , len ( tag_complete ), dropout = dropout ) self . network . to ( self . device ) self . validation_batch_size = validation_batch_size self . num_workers = num_workers self . train_losses = [] self . valid_loss = np . nan self . quantized = False self . halved = False","title":"__init__()"},{"location":"nerda_models/#NERDA.models.NERDA.evaluate_performance","text":"Evaluate Performance Evaluates the performance of the model on an arbitrary data set. Parameters: Name Type Description Default dataset dict Data set that must consist of 'sentences' and NER'tags'. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). required kwargs arbitrary keyword arguments for predict. For instance 'batch_size' and 'num_workers'. {} return_accuracy bool Return accuracy as well? Defaults to False. False Returns: Type Description DataFrame DataFrame with performance numbers, F1-scores, Precision and Recall. Returns dictionary with this AND accuracy, if return_accuracy is set to True. Source code in NERDA/models.py def evaluate_performance ( self , dataset : dict , return_accuracy : bool = False , ** kwargs ) -> pd . DataFrame : \"\"\"Evaluate Performance Evaluates the performance of the model on an arbitrary data set. Args: dataset (dict): Data set that must consist of 'sentences' and NER'tags'. You can look at examples of, how the dataset should look like by invoking functions get_dane_data() or get_conll_data(). kwargs: arbitrary keyword arguments for predict. For instance 'batch_size' and 'num_workers'. return_accuracy (bool): Return accuracy as well? Defaults to False. Returns: DataFrame with performance numbers, F1-scores, Precision and Recall. Returns dictionary with this AND accuracy, if return_accuracy is set to True. \"\"\" tags_predicted = self . predict ( dataset . get ( 'sentences' ), ** kwargs ) # compute F1 scores by entity type f1 = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = None ) # create DataFrame with performance scores (=F1) df = list ( zip ( self . tag_scheme , f1 [ 2 ], f1 [ 0 ], f1 [ 1 ])) df = pd . DataFrame ( df , columns = [ 'Level' , 'F1-Score' , 'Precision' , 'Recall' ]) # compute MICRO-averaged F1-scores and add to table. f1_micro = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = 'micro' ) f1_micro = pd . DataFrame ({ 'Level' : [ 'AVG_MICRO' ], 'F1-Score' : [ f1_micro [ 2 ]], 'Precision' : [ np . nan ], 'Recall' : [ np . nan ]}) df = df . append ( f1_micro ) # compute MACRO-averaged F1-scores and add to table. f1_macro = compute_f1_scores ( y_pred = tags_predicted , y_true = dataset . get ( 'tags' ), labels = self . tag_scheme , average = 'macro' ) f1_macro = pd . DataFrame ({ 'Level' : [ 'AVG_MICRO' ], 'F1-Score' : [ f1_macro [ 2 ]], 'Precision' : [ np . nan ], 'Recall' : [ np . nan ]}) df = df . append ( f1_macro ) # compute and return accuracy if desired if return_accuracy : accuracy = accuracy_score ( y_pred = flatten ( tags_predicted ), y_true = flatten ( dataset . get ( 'tags' ))) return { 'f1' : df , 'accuracy' : accuracy } return df","title":"evaluate_performance()"},{"location":"nerda_models/#NERDA.models.NERDA.half","text":"Convert weights from Float32 to Float16 to increase performance Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=half#torch.nn.Module.half Returns: Nothing. Model is \"halved\" as a side-effect. Source code in NERDA/models.py def half ( self ): \"\"\"Convert weights from Float32 to Float16 to increase performance Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/docs/master/generated/torch.nn.Module.html?highlight=half#torch.nn.Module.half Returns: Nothing. Model is \"halved\" as a side-effect. \"\"\" assert not ( self . halved ), \"Half precision already applied\" assert not ( self . quantized ), \"Can't run both quantization and half precision\" self . network . half () self . halved = True","title":"half()"},{"location":"nerda_models/#NERDA.models.NERDA.load_network_from_file","text":"Load Pretrained NERDA Network from file Loads weights for a pretrained NERDA Network from file. Parameters: Name Type Description Default model_path str Path for model file. Defaults to \"model.bin\". 'model.bin' Returns: Type Description str str: message telling if weights for network were loaded succesfully. Source code in NERDA/models.py def load_network_from_file ( self , model_path = \"model.bin\" ) -> str : \"\"\"Load Pretrained NERDA Network from file Loads weights for a pretrained NERDA Network from file. Args: model_path (str, optional): Path for model file. Defaults to \"model.bin\". Returns: str: message telling if weights for network were loaded succesfully. \"\"\" # TODO: change assert to Raise. assert os . path . exists ( model_path ), \"File does not exist. You can download network with download_network()\" self . network . load_state_dict ( torch . load ( model_path , map_location = torch . device ( self . device ))) self . network . device = self . device return f 'Weights for network loaded from { model_path } '","title":"load_network_from_file()"},{"location":"nerda_models/#NERDA.models.NERDA.predict","text":"Predict Named Entities in Word-Tokenized Sentences Predicts word-tokenized sentences with trained model. Parameters: Name Type Description Default sentences List[List[str]] word-tokenized sentences. required kwargs arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. {} return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False Returns: Type Description List[List[str]] List[List[str]]: Predicted tags for sentences - one predicted tag/entity per word token. Source code in NERDA/models.py def predict ( self , sentences : List [ List [ str ]], return_confidence : bool = False , ** kwargs ) -> List [ List [ str ]]: \"\"\"Predict Named Entities in Word-Tokenized Sentences Predicts word-tokenized sentences with trained model. Args: sentences (List[List[str]]): word-tokenized sentences. kwargs: arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. Returns: List[List[str]]: Predicted tags for sentences - one predicted tag/entity per word token. \"\"\" return predict ( network = self . network , sentences = sentences , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , max_len = self . max_len , device = self . device , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , return_confidence = return_confidence , ** kwargs )","title":"predict()"},{"location":"nerda_models/#NERDA.models.NERDA.predict_text","text":"Predict Named Entities in a Text Parameters: Name Type Description Default text str text to predict entities in. required kwargs arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. {} return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False Returns: Type Description list tuple: word-tokenized sentences and predicted tags/entities. Source code in NERDA/models.py def predict_text ( self , text : str , return_confidence : bool = False , ** kwargs ) -> list : \"\"\"Predict Named Entities in a Text Args: text (str): text to predict entities in. kwargs: arbitrary keyword arguments. For instance 'batch_size' and 'num_workers'. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. Returns: tuple: word-tokenized sentences and predicted tags/entities. \"\"\" return predict_text ( network = self . network , text = text , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , max_len = self . max_len , device = self . device , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , return_confidence = return_confidence , ** kwargs )","title":"predict_text()"},{"location":"nerda_models/#NERDA.models.NERDA.quantize","text":"Apply dynamic quantization to increase performance. Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html Returns: Type Description Nothing. Applies dynamic quantization to Network as a side-effect. Source code in NERDA/models.py def quantize ( self ): \"\"\"Apply dynamic quantization to increase performance. Quantization and half precision inference are mutually exclusive. Read more: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html Returns: Nothing. Applies dynamic quantization to Network as a side-effect. \"\"\" assert not ( self . quantized ), \"Dynamic quantization already applied\" assert not ( self . halved ), \"Can't run both quantization and half precision\" self . network = torch . quantization . quantize_dynamic ( self . network , { torch . nn . Linear }, dtype = torch . qint8 ) self . quantized = True","title":"quantize()"},{"location":"nerda_models/#NERDA.models.NERDA.save_network","text":"Save Weights of NERDA Network Saves weights for a fine-tuned NERDA Network to file. Parameters: Name Type Description Default model_path str Path for model file. Defaults to \"model.bin\". 'model.bin' Returns: Type Description None Nothing. Saves model to file as a side-effect. Source code in NERDA/models.py def save_network ( self , model_path : str = \"model.bin\" ) -> None : \"\"\"Save Weights of NERDA Network Saves weights for a fine-tuned NERDA Network to file. Args: model_path (str, optional): Path for model file. Defaults to \"model.bin\". Returns: Nothing. Saves model to file as a side-effect. \"\"\" torch . save ( self . network . state_dict (), model_path ) print ( f \"Network written to file { model_path } \" )","title":"save_network()"},{"location":"nerda_models/#NERDA.models.NERDA.train","text":"Train Network Trains the network from the NERDA model specification. Returns: Type Description str str: a message saying if the model was trained succesfully. The network in the 'network' attribute is trained as a side-effect. Training losses and validation loss are saved in 'training_losses' and 'valid_loss' attributes respectively as side-effects. Source code in NERDA/models.py def train ( self ) -> str : \"\"\"Train Network Trains the network from the NERDA model specification. Returns: str: a message saying if the model was trained succesfully. The network in the 'network' attribute is trained as a side-effect. Training losses and validation loss are saved in 'training_losses' and 'valid_loss' attributes respectively as side-effects. \"\"\" network , train_losses , valid_loss = train_model ( network = self . network , tag_encoder = self . tag_encoder , tag_outside = self . tag_outside , transformer_tokenizer = self . transformer_tokenizer , transformer_config = self . transformer_config , dataset_training = self . dataset_training , dataset_validation = self . dataset_validation , validation_batch_size = self . validation_batch_size , max_len = self . max_len , device = self . device , num_workers = self . num_workers , ** self . hyperparameters ) # attach as attributes to class setattr ( self , \"network\" , network ) setattr ( self , \"train_losses\" , train_losses ) setattr ( self , \"valid_loss\" , valid_loss ) return \"Model trained successfully\"","title":"train()"},{"location":"networks/","text":"Networks This section covers torch networks for NERDA NERDANetwork A Generic Network for NERDA models. The network has an analogous architecture to the models in Hvingelby et al. 2020 . Can be replaced with a custom user-defined network with the restriction, that it must take the same arguments. __init__ ( self , transformer , device , n_tags , dropout = 0.1 ) special Initialize a NERDA Network Parameters: Name Type Description Default transformer Module huggingface torch transformer. required device str Computational device. required n_tags int Number of unique entity tags (incl. outside tag) required dropout float Dropout probability. Defaults to 0.1. 0.1 Source code in NERDA/networks.py def __init__ ( self , transformer : nn . Module , device : str , n_tags : int , dropout : float = 0.1 ) -> None : \"\"\"Initialize a NERDA Network Args: transformer (nn.Module): huggingface `torch` transformer. device (str): Computational device. n_tags (int): Number of unique entity tags (incl. outside tag) dropout (float, optional): Dropout probability. Defaults to 0.1. \"\"\" super ( NERDANetwork , self ) . __init__ () # extract transformer name transformer_name = transformer . name_or_path # extract AutoConfig, from which relevant parameters can be extracted. transformer_config = AutoConfig . from_pretrained ( transformer_name ) self . transformer = transformer self . dropout = nn . Dropout ( dropout ) self . tags = nn . Linear ( transformer_config . hidden_size , n_tags ) self . device = device forward ( self , input_ids , masks , token_type_ids , target_tags , offsets ) Model Forward Iteration Parameters: Name Type Description Default input_ids Tensor Input IDs. required masks Tensor Attention Masks. required token_type_ids Tensor Token Type IDs. required target_tags Tensor Target tags. Are not used in model as-is, but they are expected downstream, so they can not be left out. required offsets Tensor Offsets to keep track of original words. Are not used in model as-is, but they are expected as down-stream, so they can not be left out. required Returns: Type Description Tensor torch.Tensor: predicted values. Source code in NERDA/networks.py def forward ( self , input_ids : torch . Tensor , masks : torch . Tensor , token_type_ids : torch . Tensor , target_tags : torch . Tensor , offsets : torch . Tensor ) -> torch . Tensor : \"\"\"Model Forward Iteration Args: input_ids (torch.Tensor): Input IDs. masks (torch.Tensor): Attention Masks. token_type_ids (torch.Tensor): Token Type IDs. target_tags (torch.Tensor): Target tags. Are not used in model as-is, but they are expected downstream, so they can not be left out. offsets (torch.Tensor): Offsets to keep track of original words. Are not used in model as-is, but they are expected as down-stream, so they can not be left out. Returns: torch.Tensor: predicted values. \"\"\" # TODO: can be improved with ** and move everything to device in a # single step. transformer_inputs = { 'input_ids' : input_ids . to ( self . device ), 'masks' : masks . to ( self . device ), 'token_type_ids' : token_type_ids . to ( self . device ) } # match args with transformer transformer_inputs = match_kwargs ( self . transformer . forward , ** transformer_inputs ) outputs = self . transformer ( ** transformer_inputs )[ 0 ] # apply drop-out outputs = self . dropout ( outputs ) # outputs for all labels/tags outputs = self . tags ( outputs ) return outputs","title":"Networks"},{"location":"networks/#networks","text":"This section covers torch networks for NERDA","title":"Networks"},{"location":"networks/#NERDA.networks.NERDANetwork","text":"A Generic Network for NERDA models. The network has an analogous architecture to the models in Hvingelby et al. 2020 . Can be replaced with a custom user-defined network with the restriction, that it must take the same arguments.","title":"NERDANetwork"},{"location":"networks/#NERDA.networks.NERDANetwork.__init__","text":"Initialize a NERDA Network Parameters: Name Type Description Default transformer Module huggingface torch transformer. required device str Computational device. required n_tags int Number of unique entity tags (incl. outside tag) required dropout float Dropout probability. Defaults to 0.1. 0.1 Source code in NERDA/networks.py def __init__ ( self , transformer : nn . Module , device : str , n_tags : int , dropout : float = 0.1 ) -> None : \"\"\"Initialize a NERDA Network Args: transformer (nn.Module): huggingface `torch` transformer. device (str): Computational device. n_tags (int): Number of unique entity tags (incl. outside tag) dropout (float, optional): Dropout probability. Defaults to 0.1. \"\"\" super ( NERDANetwork , self ) . __init__ () # extract transformer name transformer_name = transformer . name_or_path # extract AutoConfig, from which relevant parameters can be extracted. transformer_config = AutoConfig . from_pretrained ( transformer_name ) self . transformer = transformer self . dropout = nn . Dropout ( dropout ) self . tags = nn . Linear ( transformer_config . hidden_size , n_tags ) self . device = device","title":"__init__()"},{"location":"networks/#NERDA.networks.NERDANetwork.forward","text":"Model Forward Iteration Parameters: Name Type Description Default input_ids Tensor Input IDs. required masks Tensor Attention Masks. required token_type_ids Tensor Token Type IDs. required target_tags Tensor Target tags. Are not used in model as-is, but they are expected downstream, so they can not be left out. required offsets Tensor Offsets to keep track of original words. Are not used in model as-is, but they are expected as down-stream, so they can not be left out. required Returns: Type Description Tensor torch.Tensor: predicted values. Source code in NERDA/networks.py def forward ( self , input_ids : torch . Tensor , masks : torch . Tensor , token_type_ids : torch . Tensor , target_tags : torch . Tensor , offsets : torch . Tensor ) -> torch . Tensor : \"\"\"Model Forward Iteration Args: input_ids (torch.Tensor): Input IDs. masks (torch.Tensor): Attention Masks. token_type_ids (torch.Tensor): Token Type IDs. target_tags (torch.Tensor): Target tags. Are not used in model as-is, but they are expected downstream, so they can not be left out. offsets (torch.Tensor): Offsets to keep track of original words. Are not used in model as-is, but they are expected as down-stream, so they can not be left out. Returns: torch.Tensor: predicted values. \"\"\" # TODO: can be improved with ** and move everything to device in a # single step. transformer_inputs = { 'input_ids' : input_ids . to ( self . device ), 'masks' : masks . to ( self . device ), 'token_type_ids' : token_type_ids . to ( self . device ) } # match args with transformer transformer_inputs = match_kwargs ( self . transformer . forward , ** transformer_inputs ) outputs = self . transformer ( ** transformer_inputs )[ 0 ] # apply drop-out outputs = self . dropout ( outputs ) # outputs for all labels/tags outputs = self . tags ( outputs ) return outputs","title":"forward()"},{"location":"performance/","text":"Performance This section covers functionality for computing performance for NERDA.models.NERDA models. compute_f1_scores ( y_pred , y_true , labels , ** kwargs ) Compute F1 scores. Computes F1 Scores Parameters: Name Type Description Default y_pred List[List[str]] predicted values. required y_true List[List[str]] observed/true values. required labels List[str] all possible tags. required kwargs all optional arguments for precision/recall function. {} Returns: Type Description list list: resulting F1 scores. Source code in NERDA/performance.py def compute_f1_scores ( y_pred : List [ List [ str ]], y_true : List [ List [ str ]], labels : List [ str ], ** kwargs ) -> list : \"\"\"Compute F1 scores. Computes F1 Scores Args: y_pred (List): predicted values. y_true (List): observed/true values. labels (List): all possible tags. kwargs: all optional arguments for precision/recall function. Returns: list: resulting F1 scores. \"\"\" # check inputs. assert sum ([ len ( t ) < len ( p ) for t , p in zip ( y_true , y_pred )]) == 0 , \"Length of predictions must not exceed length of observed values\" # check, if some lengths of observed values exceed predicted values. n_exceeds = sum ([ len ( t ) > len ( p ) for t , p in zip ( y_true , y_pred )]) if n_exceeds > 0 : warnings . warn ( f 'length of observed values exceeded lengths of predicted values in { n_exceeds } cases and were truncated. _Consider_ increasing max_len parameter for your model.' ) # truncate observed values dimensions to match predicted values, # this is needed if predictions have been truncated earlier in # the flow. y_true = [ t [: len ( p )] for t , p in zip ( y_true , y_pred )] y_pred = flatten ( y_pred ) y_true = flatten ( y_true ) f1_scores = precision_recall_fscore_support ( y_true = y_true , y_pred = y_pred , labels = labels , ** kwargs ) return f1_scores flatten ( l ) Flattens list Source code in NERDA/performance.py def flatten ( l : list ): \"\"\"Flattens list\"\"\" return [ item for sublist in l for item in sublist ]","title":"Performance"},{"location":"performance/#performance","text":"This section covers functionality for computing performance for NERDA.models.NERDA models.","title":"Performance"},{"location":"performance/#NERDA.performance.compute_f1_scores","text":"Compute F1 scores. Computes F1 Scores Parameters: Name Type Description Default y_pred List[List[str]] predicted values. required y_true List[List[str]] observed/true values. required labels List[str] all possible tags. required kwargs all optional arguments for precision/recall function. {} Returns: Type Description list list: resulting F1 scores. Source code in NERDA/performance.py def compute_f1_scores ( y_pred : List [ List [ str ]], y_true : List [ List [ str ]], labels : List [ str ], ** kwargs ) -> list : \"\"\"Compute F1 scores. Computes F1 Scores Args: y_pred (List): predicted values. y_true (List): observed/true values. labels (List): all possible tags. kwargs: all optional arguments for precision/recall function. Returns: list: resulting F1 scores. \"\"\" # check inputs. assert sum ([ len ( t ) < len ( p ) for t , p in zip ( y_true , y_pred )]) == 0 , \"Length of predictions must not exceed length of observed values\" # check, if some lengths of observed values exceed predicted values. n_exceeds = sum ([ len ( t ) > len ( p ) for t , p in zip ( y_true , y_pred )]) if n_exceeds > 0 : warnings . warn ( f 'length of observed values exceeded lengths of predicted values in { n_exceeds } cases and were truncated. _Consider_ increasing max_len parameter for your model.' ) # truncate observed values dimensions to match predicted values, # this is needed if predictions have been truncated earlier in # the flow. y_true = [ t [: len ( p )] for t , p in zip ( y_true , y_pred )] y_pred = flatten ( y_pred ) y_true = flatten ( y_true ) f1_scores = precision_recall_fscore_support ( y_true = y_true , y_pred = y_pred , labels = labels , ** kwargs ) return f1_scores","title":"compute_f1_scores()"},{"location":"performance/#NERDA.performance.flatten","text":"Flattens list Source code in NERDA/performance.py def flatten ( l : list ): \"\"\"Flattens list\"\"\" return [ item for sublist in l for item in sublist ]","title":"flatten()"},{"location":"precooked_models/","text":"Precooked NERDA models This sections covers NERDA Models that have been 'precooked' by Ekstra Bladet and are publicly available for download. DA_BERT_ML NERDA Multilingual BERT for Danish Finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_BERT_ML () >>> model = DA_BERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]]) __init__ ( self , device = None ) special Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'bert-base-multilingual-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True }) DA_DISTILBERT_ML NERDA Multilingual BERT for Danish Finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_DISTILBERT_ML () >>> model = DA_DISTILBERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]]) __init__ ( self , device = None ) special Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'distilbert-base-multilingual-cased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : False }) DA_ELECTRA_DA NERDA Danish ELECTRA for Danish finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_ELECTRA_DA () >>> model = DA_ELECTRA_DA () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]]) __init__ ( self , device = None ) special Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'Maltehb/-l-ctra-danish-electra-small-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 5 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True }) EN_BERT_ML NERDA Multilingual BERT for English finetuned on CoNLL-2003 data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import EN_BERT_ML () >>> model = EN_BERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Old MacDonald had a farm' >>> model . predict_text ( text ) ([[ 'Old' , 'MacDonald' , 'had' , 'a' , 'farm' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]]) __init__ ( self , device = None ) special Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'bert-base-multilingual-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True }) EN_ELECTRA_EN NERDA English ELECTRA for English finetuned on CoNLL-2003 data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import EN_ELECTRA_EN () >>> model = EN_ELECTRA_EN () >>> model . download_network () >>> model . load_network () >>> text = 'Old MacDonald had a farm' >>> model . predict_text ( text ) ([[ 'Old' , 'MacDonald' , 'had' , 'a' , 'farm' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]]) __init__ ( self , device = None ) special Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'google/electra-small-discriminator' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 250 , 'train_batch_size' : 13 , 'learning_rate' : 8e-05 }, tokenizer_parameters = { 'do_lower_case' : True }) Precooked Precooked NERDA Model NERDA model specification that has been precooked/pretrained and is available for download. Inherits from NERDA.models.NERDA . __init__ ( self , ** kwargs ) special Initialize Precooked NERDA Model Parameters: Name Type Description Default kwargs all arguments for NERDA Model. {} Source code in NERDA/precooked.py def __init__ ( self , ** kwargs ) -> None : \"\"\"Initialize Precooked NERDA Model Args: kwargs: all arguments for NERDA Model. \"\"\" super () . __init__ ( ** kwargs ) download_network ( self , dir = None ) Download Precooked Network from Web Parameters: Name Type Description Default dir str Directory where the model file will be saved. Defaults to None, in which case the model will be saved in a folder '.nerda' in your home directory. None Returns: Type Description None str: Message saying if the download was successfull. Model is downloaded as a side-effect. Source code in NERDA/precooked.py def download_network ( self , dir = None ) -> None : \"\"\"Download Precooked Network from Web Args: dir (str, optional): Directory where the model file will be saved. Defaults to None, in which case the model will be saved in a folder '.nerda' in your home directory. Returns: str: Message saying if the download was successfull. Model is downloaded as a side-effect. \"\"\" model_name = type ( self ) . __name__ # url for public S3 bucket with NERDA models. url_s3 = 'https://nerda.s3-eu-west-1.amazonaws.com' url_model = f ' { url_s3 } / { model_name } .bin' if dir is None : dir = os . path . join ( str ( Path . home ()), '.nerda' ) if not os . path . exists ( dir ): os . mkdir ( dir ) file_path = os . path . join ( dir , f ' { model_name } .bin' ) print ( \"\"\" Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. \"\"\" ) print ( f 'Downloading { url_model } to { file_path } ' ) urllib . request . urlretrieve ( url_model , file_path , show_progress ) return \"Network downloaded successfully. Load network with 'load_network'.\" load_network ( self , file_path = None ) Load Pretrained Network Loads pretrained network from file. Parameters: Name Type Description Default file_path str Path to model file. Defaults to None, in which case, the function points to the '.nerda' folder the home directory. None Source code in NERDA/precooked.py def load_network ( self , file_path : str = None ) -> None : \"\"\"Load Pretrained Network Loads pretrained network from file. Args: file_path (str, optional): Path to model file. Defaults to None, in which case, the function points to the '.nerda' folder the home directory. \"\"\" model_name = type ( self ) . __name__ if file_path is None : file_path = os . path . join ( str ( Path . home ()), '.nerda' , f ' { model_name } .bin' ) assert os . path . exists ( file_path ), \"File does not exist! You can download network with download_network()\" print ( \"\"\" Model loaded. Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. \"\"\" ) self . load_network_from_file ( file_path )","title":"Precooked NERDA Models"},{"location":"precooked_models/#precooked-nerda-models","text":"This sections covers NERDA Models that have been 'precooked' by Ekstra Bladet and are publicly available for download.","title":"Precooked NERDA models"},{"location":"precooked_models/#NERDA.precooked.DA_BERT_ML","text":"NERDA Multilingual BERT for Danish Finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_BERT_ML () >>> model = DA_BERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]])","title":"DA_BERT_ML"},{"location":"precooked_models/#NERDA.precooked.DA_BERT_ML.__init__","text":"Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'bert-base-multilingual-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True })","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.DA_DISTILBERT_ML","text":"NERDA Multilingual BERT for Danish Finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_DISTILBERT_ML () >>> model = DA_DISTILBERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]])","title":"DA_DISTILBERT_ML"},{"location":"precooked_models/#NERDA.precooked.DA_DISTILBERT_ML.__init__","text":"Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'distilbert-base-multilingual-cased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : False })","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.DA_ELECTRA_DA","text":"NERDA Danish ELECTRA for Danish finetuned on DaNE data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import DA_ELECTRA_DA () >>> model = DA_ELECTRA_DA () >>> model . download_network () >>> model . load_network () >>> text = 'Jens Hansen har en bondeg\u00e5rd' >>> model . predict_text ( text ) ([[ 'Jens' , 'Hansen' , 'har' , 'en' , 'bondeg\u00e5rd' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]])","title":"DA_ELECTRA_DA"},{"location":"precooked_models/#NERDA.precooked.DA_ELECTRA_DA.__init__","text":"Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'Maltehb/-l-ctra-danish-electra-small-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 5 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True })","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.EN_BERT_ML","text":"NERDA Multilingual BERT for English finetuned on CoNLL-2003 data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import EN_BERT_ML () >>> model = EN_BERT_ML () >>> model . download_network () >>> model . load_network () >>> text = 'Old MacDonald had a farm' >>> model . predict_text ( text ) ([[ 'Old' , 'MacDonald' , 'had' , 'a' , 'farm' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]])","title":"EN_BERT_ML"},{"location":"precooked_models/#NERDA.precooked.EN_BERT_ML.__init__","text":"Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'bert-base-multilingual-uncased' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 500 , 'train_batch_size' : 13 , 'learning_rate' : 0.0001 }, tokenizer_parameters = { 'do_lower_case' : True })","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.EN_ELECTRA_EN","text":"NERDA English ELECTRA for English finetuned on CoNLL-2003 data set . Inherits from NERDA.precooked.Precooked . Examples: >>> from NERDA.precooked import EN_ELECTRA_EN () >>> model = EN_ELECTRA_EN () >>> model . download_network () >>> model . load_network () >>> text = 'Old MacDonald had a farm' >>> model . predict_text ( text ) ([[ 'Old' , 'MacDonald' , 'had' , 'a' , 'farm' ]], [[ 'B-PER' , 'I-PER' , 'O' , 'O' , 'O' ]])","title":"EN_ELECTRA_EN"},{"location":"precooked_models/#NERDA.precooked.EN_ELECTRA_EN.__init__","text":"Initialize model Source code in NERDA/precooked.py def __init__ ( self , device : str = None ) -> None : \"\"\"Initialize model\"\"\" super () . __init__ ( transformer = 'google/electra-small-discriminator' , device = device , tag_scheme = [ 'B-PER' , 'I-PER' , 'B-ORG' , 'I-ORG' , 'B-LOC' , 'I-LOC' , 'B-MISC' , 'I-MISC' ], tag_outside = 'O' , max_len = 128 , dropout = 0.1 , hyperparameters = { 'epochs' : 4 , 'warmup_steps' : 250 , 'train_batch_size' : 13 , 'learning_rate' : 8e-05 }, tokenizer_parameters = { 'do_lower_case' : True })","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.Precooked","text":"Precooked NERDA Model NERDA model specification that has been precooked/pretrained and is available for download. Inherits from NERDA.models.NERDA .","title":"Precooked"},{"location":"precooked_models/#NERDA.precooked.Precooked.__init__","text":"Initialize Precooked NERDA Model Parameters: Name Type Description Default kwargs all arguments for NERDA Model. {} Source code in NERDA/precooked.py def __init__ ( self , ** kwargs ) -> None : \"\"\"Initialize Precooked NERDA Model Args: kwargs: all arguments for NERDA Model. \"\"\" super () . __init__ ( ** kwargs )","title":"__init__()"},{"location":"precooked_models/#NERDA.precooked.Precooked.download_network","text":"Download Precooked Network from Web Parameters: Name Type Description Default dir str Directory where the model file will be saved. Defaults to None, in which case the model will be saved in a folder '.nerda' in your home directory. None Returns: Type Description None str: Message saying if the download was successfull. Model is downloaded as a side-effect. Source code in NERDA/precooked.py def download_network ( self , dir = None ) -> None : \"\"\"Download Precooked Network from Web Args: dir (str, optional): Directory where the model file will be saved. Defaults to None, in which case the model will be saved in a folder '.nerda' in your home directory. Returns: str: Message saying if the download was successfull. Model is downloaded as a side-effect. \"\"\" model_name = type ( self ) . __name__ # url for public S3 bucket with NERDA models. url_s3 = 'https://nerda.s3-eu-west-1.amazonaws.com' url_model = f ' { url_s3 } / { model_name } .bin' if dir is None : dir = os . path . join ( str ( Path . home ()), '.nerda' ) if not os . path . exists ( dir ): os . mkdir ( dir ) file_path = os . path . join ( dir , f ' { model_name } .bin' ) print ( \"\"\" Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. \"\"\" ) print ( f 'Downloading { url_model } to { file_path } ' ) urllib . request . urlretrieve ( url_model , file_path , show_progress ) return \"Network downloaded successfully. Load network with 'load_network'.\"","title":"download_network()"},{"location":"precooked_models/#NERDA.precooked.Precooked.load_network","text":"Load Pretrained Network Loads pretrained network from file. Parameters: Name Type Description Default file_path str Path to model file. Defaults to None, in which case, the function points to the '.nerda' folder the home directory. None Source code in NERDA/precooked.py def load_network ( self , file_path : str = None ) -> None : \"\"\"Load Pretrained Network Loads pretrained network from file. Args: file_path (str, optional): Path to model file. Defaults to None, in which case, the function points to the '.nerda' folder the home directory. \"\"\" model_name = type ( self ) . __name__ if file_path is None : file_path = os . path . join ( str ( Path . home ()), '.nerda' , f ' { model_name } .bin' ) assert os . path . exists ( file_path ), \"File does not exist! You can download network with download_network()\" print ( \"\"\" Model loaded. Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. \"\"\" ) self . load_network_from_file ( file_path )","title":"load_network()"},{"location":"predictions/","text":"Predictions This section covers functionality for computing predictions with a NERDA.models.NERDA model. predict ( network , sentences , transformer_tokenizer , transformer_config , max_len , device , tag_encoder , tag_outside , batch_size = 8 , num_workers = 1 , return_tensors = False , return_confidence = False , pad_sequences = True ) Compute predictions. Computes predictions for a list with word-tokenized sentences with a NERDA model. Parameters: Name Type Description Default network Module Network. required sentences List[List[str]] List of lists with word-tokenized sentences. required transformer_tokenizer PreTrainedTokenizer tokenizer for transformer model. required transformer_config PretrainedConfig config for transformer model. required max_len int Maximum length of sentence after applying transformer tokenizer. required device str Computational device. required tag_encoder LabelEncoder Encoder for Named-Entity tags. required tag_outside str Special 'outside' NER tag. required batch_size int Batch Size for DataLoader. Defaults to 8. 8 num_workers int Number of workers. Defaults to 1. 1 return_tensors bool if True, return tensors. False return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False pad_sequences bool if True, pad sequences. Defaults to True. True Returns: Type Description List[List[str]] List[List[str]]: List of lists with predicted Entity tags. Source code in NERDA/predictions.py def predict ( network : torch . nn . Module , sentences : List [ List [ str ]], transformer_tokenizer : transformers . PreTrainedTokenizer , transformer_config : transformers . PretrainedConfig , max_len : int , device : str , tag_encoder : sklearn . preprocessing . LabelEncoder , tag_outside : str , batch_size : int = 8 , num_workers : int = 1 , return_tensors : bool = False , return_confidence : bool = False , pad_sequences : bool = True ) -> List [ List [ str ]]: \"\"\"Compute predictions. Computes predictions for a list with word-tokenized sentences with a `NERDA` model. Args: network (torch.nn.Module): Network. sentences (List[List[str]]): List of lists with word-tokenized sentences. transformer_tokenizer (transformers.PreTrainedTokenizer): tokenizer for transformer model. transformer_config (transformers.PretrainedConfig): config for transformer model. max_len (int): Maximum length of sentence after applying transformer tokenizer. device (str): Computational device. tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder for Named-Entity tags. tag_outside (str): Special 'outside' NER tag. batch_size (int, optional): Batch Size for DataLoader. Defaults to 8. num_workers (int, optional): Number of workers. Defaults to 1. return_tensors (bool, optional): if True, return tensors. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. pad_sequences (bool, optional): if True, pad sequences. Defaults to True. Returns: List[List[str]]: List of lists with predicted Entity tags. \"\"\" # make sure, that input has the correct format. assert isinstance ( sentences , list ), \"'sentences' must be a list of list of word-tokens\" assert isinstance ( sentences [ 0 ], list ), \"'sentences' must be a list of list of word-tokens\" assert isinstance ( sentences [ 0 ][ 0 ], str ), \"'sentences' must be a list of list of word-tokens\" # set network to appropriate mode. network . eval () # fill 'dummy' tags (expected input for dataloader). tag_fill = [ tag_encoder . classes_ [ 0 ]] tags_dummy = [ tag_fill * len ( sent ) for sent in sentences ] dl = create_dataloader ( sentences = sentences , tags = tags_dummy , transformer_tokenizer = transformer_tokenizer , transformer_config = transformer_config , max_len = max_len , batch_size = batch_size , tag_encoder = tag_encoder , tag_outside = tag_outside , num_workers = num_workers , pad_sequences = pad_sequences ) predictions = [] probabilities = [] tensors = [] with torch . no_grad (): for _ , dl in enumerate ( dl ): outputs = network ( ** dl ) # conduct operations on sentence level. for i in range ( outputs . shape [ 0 ]): # extract prediction and transform. # find max by row. values , indices = outputs [ i ] . max ( dim = 1 ) preds = tag_encoder . inverse_transform ( indices . cpu () . numpy ()) probs = values . cpu () . numpy () if return_tensors : tensors . append ( outputs ) # subset predictions for original word tokens. preds = [ prediction for prediction , offset in zip ( preds . tolist (), dl . get ( 'offsets' )[ i ]) if offset ] if return_confidence : probs = [ prob for prob , offset in zip ( probs . tolist (), dl . get ( 'offsets' )[ i ]) if offset ] # Remove special tokens ('CLS' + 'SEP'). preds = preds [ 1 : - 1 ] if return_confidence : probs = probs [ 1 : - 1 ] # make sure resulting predictions have same length as # original sentence. # TODO: Move assert statement to unit tests. Does not work # in boundary. # assert len(preds) == len(sentences[i]) predictions . append ( preds ) if return_confidence : probabilities . append ( probs ) if return_confidence : return predictions , probabilities if return_tensors : return tensors return predictions predict_text ( network , text , transformer_tokenizer , transformer_config , max_len , device , tag_encoder , tag_outside , batch_size = 8 , num_workers = 1 , pad_sequences = True , return_confidence = False , sent_tokenize =< function sent_tokenize at 0x7f69593ace60 > , word_tokenize =< function word_tokenize at 0x7f695916ddd0 > ) Compute Predictions for Text. Computes predictions for a text with NERDA model. Text is tokenized into sentences before computing predictions. Parameters: Name Type Description Default network Module Network. required text str text to predict entities in. required transformer_tokenizer PreTrainedTokenizer tokenizer for transformer model. required transformer_config PretrainedConfig config for transformer model. required max_len int Maximum length of sentence after applying transformer tokenizer. required device str Computational device. required tag_encoder LabelEncoder Encoder for Named-Entity tags. required tag_outside str Special 'outside' NER tag. required batch_size int Batch Size for DataLoader. Defaults to 8. 8 num_workers int Number of workers. Defaults to 1. 1 pad_sequences bool if True, pad sequences. Defaults to True. True return_confidence bool if True, return confidence scores for predicted tokens. Defaults to False. False Returns: Type Description tuple tuple: sentence- and word-tokenized text with corresponding predicted named-entity tags. Source code in NERDA/predictions.py def predict_text ( network : torch . nn . Module , text : str , transformer_tokenizer : transformers . PreTrainedTokenizer , transformer_config : transformers . PretrainedConfig , max_len : int , device : str , tag_encoder : sklearn . preprocessing . LabelEncoder , tag_outside : str , batch_size : int = 8 , num_workers : int = 1 , pad_sequences : bool = True , return_confidence : bool = False , sent_tokenize : Callable = sent_tokenize , word_tokenize : Callable = word_tokenize ) -> tuple : \"\"\"Compute Predictions for Text. Computes predictions for a text with `NERDA` model. Text is tokenized into sentences before computing predictions. Args: network (torch.nn.Module): Network. text (str): text to predict entities in. transformer_tokenizer (transformers.PreTrainedTokenizer): tokenizer for transformer model. transformer_config (transformers.PretrainedConfig): config for transformer model. max_len (int): Maximum length of sentence after applying transformer tokenizer. device (str): Computational device. tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder for Named-Entity tags. tag_outside (str): Special 'outside' NER tag. batch_size (int, optional): Batch Size for DataLoader. Defaults to 8. num_workers (int, optional): Number of workers. Defaults to 1. pad_sequences (bool, optional): if True, pad sequences. Defaults to True. return_confidence (bool, optional): if True, return confidence scores for predicted tokens. Defaults to False. Returns: tuple: sentence- and word-tokenized text with corresponding predicted named-entity tags. \"\"\" assert isinstance ( text , str ), \"'text' must be a string.\" sentences = sent_tokenize ( text ) sentences = [ word_tokenize ( sentence ) for sentence in sentences ] predictions = predict ( network = network , sentences = sentences , transformer_tokenizer = transformer_tokenizer , transformer_config = transformer_config , max_len = max_len , device = device , return_confidence = return_confidence , batch_size = batch_size , num_workers = num_workers , pad_sequences = pad_sequences , tag_encoder = tag_encoder , tag_outside = tag_outside ) return sentences , predictions","title":"Predictions"},{"location":"predictions/#predictions","text":"This section covers functionality for computing predictions with a NERDA.models.NERDA model.","title":"Predictions"},{"location":"predictions/#NERDA.predictions.predict","text":"Compute predictions. Computes predictions for a list with word-tokenized sentences with a NERDA model. Parameters: Name Type Description Default network Module Network. required sentences List[List[str]] List of lists with word-tokenized sentences. required transformer_tokenizer PreTrainedTokenizer tokenizer for transformer model. required transformer_config PretrainedConfig config for transformer model. required max_len int Maximum length of sentence after applying transformer tokenizer. required device str Computational device. required tag_encoder LabelEncoder Encoder for Named-Entity tags. required tag_outside str Special 'outside' NER tag. required batch_size int Batch Size for DataLoader. Defaults to 8. 8 num_workers int Number of workers. Defaults to 1. 1 return_tensors bool if True, return tensors. False return_confidence bool if True, return confidence scores for all predicted tokens. Defaults to False. False pad_sequences bool if True, pad sequences. Defaults to True. True Returns: Type Description List[List[str]] List[List[str]]: List of lists with predicted Entity tags. Source code in NERDA/predictions.py def predict ( network : torch . nn . Module , sentences : List [ List [ str ]], transformer_tokenizer : transformers . PreTrainedTokenizer , transformer_config : transformers . PretrainedConfig , max_len : int , device : str , tag_encoder : sklearn . preprocessing . LabelEncoder , tag_outside : str , batch_size : int = 8 , num_workers : int = 1 , return_tensors : bool = False , return_confidence : bool = False , pad_sequences : bool = True ) -> List [ List [ str ]]: \"\"\"Compute predictions. Computes predictions for a list with word-tokenized sentences with a `NERDA` model. Args: network (torch.nn.Module): Network. sentences (List[List[str]]): List of lists with word-tokenized sentences. transformer_tokenizer (transformers.PreTrainedTokenizer): tokenizer for transformer model. transformer_config (transformers.PretrainedConfig): config for transformer model. max_len (int): Maximum length of sentence after applying transformer tokenizer. device (str): Computational device. tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder for Named-Entity tags. tag_outside (str): Special 'outside' NER tag. batch_size (int, optional): Batch Size for DataLoader. Defaults to 8. num_workers (int, optional): Number of workers. Defaults to 1. return_tensors (bool, optional): if True, return tensors. return_confidence (bool, optional): if True, return confidence scores for all predicted tokens. Defaults to False. pad_sequences (bool, optional): if True, pad sequences. Defaults to True. Returns: List[List[str]]: List of lists with predicted Entity tags. \"\"\" # make sure, that input has the correct format. assert isinstance ( sentences , list ), \"'sentences' must be a list of list of word-tokens\" assert isinstance ( sentences [ 0 ], list ), \"'sentences' must be a list of list of word-tokens\" assert isinstance ( sentences [ 0 ][ 0 ], str ), \"'sentences' must be a list of list of word-tokens\" # set network to appropriate mode. network . eval () # fill 'dummy' tags (expected input for dataloader). tag_fill = [ tag_encoder . classes_ [ 0 ]] tags_dummy = [ tag_fill * len ( sent ) for sent in sentences ] dl = create_dataloader ( sentences = sentences , tags = tags_dummy , transformer_tokenizer = transformer_tokenizer , transformer_config = transformer_config , max_len = max_len , batch_size = batch_size , tag_encoder = tag_encoder , tag_outside = tag_outside , num_workers = num_workers , pad_sequences = pad_sequences ) predictions = [] probabilities = [] tensors = [] with torch . no_grad (): for _ , dl in enumerate ( dl ): outputs = network ( ** dl ) # conduct operations on sentence level. for i in range ( outputs . shape [ 0 ]): # extract prediction and transform. # find max by row. values , indices = outputs [ i ] . max ( dim = 1 ) preds = tag_encoder . inverse_transform ( indices . cpu () . numpy ()) probs = values . cpu () . numpy () if return_tensors : tensors . append ( outputs ) # subset predictions for original word tokens. preds = [ prediction for prediction , offset in zip ( preds . tolist (), dl . get ( 'offsets' )[ i ]) if offset ] if return_confidence : probs = [ prob for prob , offset in zip ( probs . tolist (), dl . get ( 'offsets' )[ i ]) if offset ] # Remove special tokens ('CLS' + 'SEP'). preds = preds [ 1 : - 1 ] if return_confidence : probs = probs [ 1 : - 1 ] # make sure resulting predictions have same length as # original sentence. # TODO: Move assert statement to unit tests. Does not work # in boundary. # assert len(preds) == len(sentences[i]) predictions . append ( preds ) if return_confidence : probabilities . append ( probs ) if return_confidence : return predictions , probabilities if return_tensors : return tensors return predictions","title":"predict()"},{"location":"predictions/#NERDA.predictions.predict_text","text":"Compute Predictions for Text. Computes predictions for a text with NERDA model. Text is tokenized into sentences before computing predictions. Parameters: Name Type Description Default network Module Network. required text str text to predict entities in. required transformer_tokenizer PreTrainedTokenizer tokenizer for transformer model. required transformer_config PretrainedConfig config for transformer model. required max_len int Maximum length of sentence after applying transformer tokenizer. required device str Computational device. required tag_encoder LabelEncoder Encoder for Named-Entity tags. required tag_outside str Special 'outside' NER tag. required batch_size int Batch Size for DataLoader. Defaults to 8. 8 num_workers int Number of workers. Defaults to 1. 1 pad_sequences bool if True, pad sequences. Defaults to True. True return_confidence bool if True, return confidence scores for predicted tokens. Defaults to False. False Returns: Type Description tuple tuple: sentence- and word-tokenized text with corresponding predicted named-entity tags. Source code in NERDA/predictions.py def predict_text ( network : torch . nn . Module , text : str , transformer_tokenizer : transformers . PreTrainedTokenizer , transformer_config : transformers . PretrainedConfig , max_len : int , device : str , tag_encoder : sklearn . preprocessing . LabelEncoder , tag_outside : str , batch_size : int = 8 , num_workers : int = 1 , pad_sequences : bool = True , return_confidence : bool = False , sent_tokenize : Callable = sent_tokenize , word_tokenize : Callable = word_tokenize ) -> tuple : \"\"\"Compute Predictions for Text. Computes predictions for a text with `NERDA` model. Text is tokenized into sentences before computing predictions. Args: network (torch.nn.Module): Network. text (str): text to predict entities in. transformer_tokenizer (transformers.PreTrainedTokenizer): tokenizer for transformer model. transformer_config (transformers.PretrainedConfig): config for transformer model. max_len (int): Maximum length of sentence after applying transformer tokenizer. device (str): Computational device. tag_encoder (sklearn.preprocessing.LabelEncoder): Encoder for Named-Entity tags. tag_outside (str): Special 'outside' NER tag. batch_size (int, optional): Batch Size for DataLoader. Defaults to 8. num_workers (int, optional): Number of workers. Defaults to 1. pad_sequences (bool, optional): if True, pad sequences. Defaults to True. return_confidence (bool, optional): if True, return confidence scores for predicted tokens. Defaults to False. Returns: tuple: sentence- and word-tokenized text with corresponding predicted named-entity tags. \"\"\" assert isinstance ( text , str ), \"'text' must be a string.\" sentences = sent_tokenize ( text ) sentences = [ word_tokenize ( sentence ) for sentence in sentences ] predictions = predict ( network = network , sentences = sentences , transformer_tokenizer = transformer_tokenizer , transformer_config = transformer_config , max_len = max_len , device = device , return_confidence = return_confidence , batch_size = batch_size , num_workers = num_workers , pad_sequences = pad_sequences , tag_encoder = tag_encoder , tag_outside = tag_outside ) return sentences , predictions","title":"predict_text()"},{"location":"workflow/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Workflow Examples NERDA offers a simple easy-to-use interface for fine-tuning transformers for Named-Entity Recognition (=NER). We call this family of models NERDA models. NERDA can be used in two ways. You can either (1) train your own customized NERDA model or (2) download and use one of our precooked NERDA models for inference i.e. identifying named entities in new texts. Train Your Own NERDA model We want to fine-tune a transformer for English. First, we download an English NER dataset CoNLL-2003 with annotated Named Entities, that we will use for training and evaluation of our model. from NERDA.datasets import get_conll_data, download_conll_data download_conll_data() Reading https://data.deepai.org/conll2003.zip 'archive extracted to /home/runner/.conll' CoNLL-2003 operates with the following types of named entities: PER sons ORG anizations LOC ations MISC ellaneous O utside (Not a named Entity) An observation from the CoNLL-2003 data set looks like this. # extract the first _5_ rows from the training and validation data splits. training = get_conll_data('train', 5) validation = get_conll_data('valid', 5) # example sentence = training.get('sentences')[0] tags = training.get('tags')[0] print(\"\\n\".join([\"{}/{}\".format(word, tag) for word, tag in zip(sentence, tags)])) EU/B-ORG rejects/O German/B-MISC call/O to/O boycott/O British/B-MISC lamb/O ./O If you provide your own dataset, it must have the same structure: It must be a dictionary The dictionary must contain 'sentences': a list of word-tokenized sentences with one sentence per entry 'tags': a list with the corresponding named-entity tags. The data set does however not have to follow the Inside-Outside-Beginning (IOB) tagging scheme 1 . The IOB tagging scheme implies, that words that are beginning of named entities are tagged with 'B-' and words 'inside' (=continuations of) named entities are tagged with 'I-' . That means that 'Joe Biden' should be tagged as Joe(B-PER) Biden(I-PER) . Now, instantiate a NERDA model for finetuning an ELECTRA transformer for NER. from NERDA.models import NERDA tag_scheme = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'] model = NERDA(dataset_training = training, dataset_validation = validation, tag_scheme = tag_scheme, tag_outside = 'O', transformer = 'google/electra-small-discriminator', hyperparameters = {'epochs' : 1, 'warmup_steps' : 10, 'train_batch_size': 5, 'learning_rate': 0.0001},) Device automatically set to: cpu var element = $('#08b62564-aab2-4f5c-be30-a694ce177498'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"09983c06919d462f83d7b70ac1da344b\"} var element = $('#b75870eb-209f-4b61-aae4-ed7155b40cd8'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"01e94a94d269403b844e06f5fad4e6d2\"} Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight'] - This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). var element = $('#c653245b-b312-4ad4-b5bc-7176b70e5822'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"93ee287f85034c688772a906c5fb4cca\"} var element = $('#cc70370a-4fd7-403c-b9e0-a612cc5d99d1'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"03b6e5d14f7d465eaaa78aa9d214deef\"} var element = $('#10b57c26-a02d-4768-803e-ed053512d3cd'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"010f83be4f6f412487e749e86f75a56b\"} Note, this model configuration only uses 5 sentences for model training to minimize execution time. Also the hyperparameters for the model have been chosen in order to minimize execution time. Therefore this example only serves to illustrate the functionality i.e. the resulting model will suck. By default the network architecture is analogous that of the models in Hvingelby et al. 2020 . The model can be trained right away by invoking the train method. model.train() Epoch 1 / 1 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.94it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.83it/s] Train Loss = 2.361093521118164 Valid Loss = 2.3556602001190186 'Model trained successfully' We can compute the performance of the model on a test set (limited to 5 sentences): test = get_conll_data('test', 5) model.evaluate_performance(test) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Level F1-Score Precision Recall 0 B-PER 0.0 0.0 0.0 1 I-PER 0.0 0.0 0.0 2 B-ORG 0.0 0.0 0.0 3 I-ORG 0.0 0.0 0.0 4 B-LOC 0.0 0.0 0.0 5 I-LOC 0.0 0.0 0.0 6 B-MISC 0.0 0.0 0.0 7 I-MISC 0.0 0.0 0.0 0 AVG_MICRO 0.0 NaN NaN 0 AVG_MICRO 0.0 NaN NaN Unsurprisingly, the model sucks in this case due to the ludicrous specification. Named Entities in new texts can be predicted with predict functions. text = \"Old MacDonald had a farm\" model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['I-PER', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC']]) Needless to say the predicted entities for this model are nonsensical. To get a more reasonable model, provide more data and a more meaningful model specification. In general NERDA has the following handles, that you use. provide your own data set choose whatever pretrained transformer you would like to fine-tune provide your own set of hyperparameters and lastly provide your own torch network (architecture). You can do this by instantiating a NERDA model with the parameter 'network' set to your own network (torch.nn.Module). Use a Precooked NERDA model We have precooked a number of NERDA models, that you can download and use right off the shelf. Here is an example. Instantiate a NERDA model based on the English ELECTRA transformer, that has been finetuned for NER in English, EN_ELECTRA_EN . from NERDA.precooked import EN_ELECTRA_EN model = EN_ELECTRA_EN() Device automatically set to: cpu Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight'] - This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). (Down)load network: model.download_network() model.load_network() Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. Downloading https://nerda.s3-eu-west-1.amazonaws.com/EN_ELECTRA_EN.bin to /home/runner/.nerda/EN_ELECTRA_EN.bin 100% |########################################################################| Model loaded. Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. This model performs much better: model.evaluate_performance(get_conll_data('test', 100)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Level F1-Score Precision Recall 0 B-PER 0.990909 1.000000 0.981982 1 I-PER 1.000000 1.000000 1.000000 2 B-ORG 0.800000 0.666667 1.000000 3 I-ORG 1.000000 1.000000 1.000000 4 B-LOC 0.993939 0.987952 1.000000 5 I-LOC 1.000000 1.000000 1.000000 6 B-MISC 0.930233 0.952381 0.909091 7 I-MISC 0.956522 1.000000 0.916667 0 AVG_MICRO 0.988060 NaN NaN 0 AVG_MICRO 0.958950 NaN NaN Predict named entities in new texts text = 'Old MacDonald had a farm' model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']]) List of Precooked Models The table below shows the precooked NERDA models publicly available for download. We have trained models for Danish and English. Model Language Transformer Dataset F1-score DA_BERT_ML Danish Multilingual BERT DaNE 82.8 DA_ELECTRA_DA Danish Danish ELECTRA DaNE 79.8 EN_BERT_ML English Multilingual BERT CoNLL-2003 90.4 EN_ELECTRA_EN English English ELECTRA CoNLL-2003 89.1 F1-score is the micro-averaged F1-score across entity tags and is evaluated on the respective test sets (that have not been used for training nor validation of the models). Note, that we have not spent a lot of time on actually fine-tuning the models, so there could be room for improvement. If you are able to improve the models, we will be happy to hear from you and include your NERDA model. Performance of Precooked Models The table below summarizes the performance as measured by F1-scores of the model configurations, that NERDA ships with. Level DA_BERT_ML DA_ELECTRA_DA EN_BERT_ML EN_ELECTRA_EN B-PER 93.8 92.0 96.0 95.1 I-PER 97.8 97.1 98.5 97.9 B-ORG 69.5 66.9 88.4 86.2 I-ORG 69.9 70.7 85.7 83.1 B-LOC 82.5 79.0 92.3 91.1 I-LOC 31.6 44.4 83.9 80.5 B-MISC 73.4 68.6 81.8 80.1 I-MISC 86.1 63.6 63.4 68.4 AVG_MICRO 82.8 79.8 90.4 89.1 AVG_MACRO 75.6 72.8 86.3 85.3 This concludes our walkthrough of NERDA . If you have any questions, please do not hesitate to contact us ! {\"state\": {\"305e4c6914644c18a13d6dd6d7bd5a55\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"3237c4384da94f139d3091d3d0bacf5b\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c4039a63550a45ffb1450f72acf95830\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_305e4c6914644c18a13d6dd6d7bd5a55\", \"max\": 665.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_3237c4384da94f139d3091d3d0bacf5b\", \"value\": 665.0}}, \"33599426d91645918d01134477344962\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8dc6711db10046f6b1cb035e72c9a2a2\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"32122a5509db462ab09448e37f496336\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_33599426d91645918d01134477344962\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_8dc6711db10046f6b1cb035e72c9a2a2\", \"value\": \"Downloading: 100%\"}}, \"664d15ae485d42bc90f2783f18f8ad53\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"9669c61537a442268e980208098eb2c5\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"91e4954d53124e5a9b5ca7a3f16482a4\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_664d15ae485d42bc90f2783f18f8ad53\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_9669c61537a442268e980208098eb2c5\", \"value\": \" 665/665 [00:00&lt;00:00, 24.8kB/s]\"}}, \"84f10b91e95445c091b10186b2246b70\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"09983c06919d462f83d7b70ac1da344b\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_32122a5509db462ab09448e37f496336\", \"IPY_MODEL_c4039a63550a45ffb1450f72acf95830\", \"IPY_MODEL_91e4954d53124e5a9b5ca7a3f16482a4\"], \"layout\": \"IPY_MODEL_84f10b91e95445c091b10186b2246b70\"}}, \"aa6e3ddcff7c462c86e7d9d8683149f6\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"435348c4408d4d3eab8746badcfafb21\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c41dc83e16c94d30a5817e3092569b5d\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_aa6e3ddcff7c462c86e7d9d8683149f6\", \"max\": 54245363.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_435348c4408d4d3eab8746badcfafb21\", \"value\": 54245363.0}}, \"c1818ad0151649978aea49edb40acd20\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"354fa859b1054490b7799b44f1378fc8\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"631c29019fc043eeb3d5152deaafc1af\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_c1818ad0151649978aea49edb40acd20\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_354fa859b1054490b7799b44f1378fc8\", \"value\": \"Downloading: 100%\"}}, \"23aaf4b17bb24cc791d05ea8c48e36f7\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"07d1cdb334944f70bccfe63d6152e580\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"45350bd1d66a42a790e1a9c2c2d035a1\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_23aaf4b17bb24cc791d05ea8c48e36f7\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_07d1cdb334944f70bccfe63d6152e580\", \"value\": \" 54.2M/54.2M [00:01&lt;00:00, 24.6MB/s]\"}}, \"1c9c8deecdd5408eb42e9e726057010b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"01e94a94d269403b844e06f5fad4e6d2\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_631c29019fc043eeb3d5152deaafc1af\", \"IPY_MODEL_c41dc83e16c94d30a5817e3092569b5d\", \"IPY_MODEL_45350bd1d66a42a790e1a9c2c2d035a1\"], \"layout\": \"IPY_MODEL_1c9c8deecdd5408eb42e9e726057010b\"}}, \"87ddfd9b1bb74b92b242e901a53c44e3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"97c62537400143ff9831d716f839877f\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"34c3bdbdf0d14d73a835e2af601201f0\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_87ddfd9b1bb74b92b242e901a53c44e3\", \"max\": 29.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_97c62537400143ff9831d716f839877f\", \"value\": 29.0}}, \"cdc083734cdc4132a52bc7f422446a81\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c2c11ff9f74a460b9956a7bb545986df\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"a194868856ff4fe28d0d07562e629d9c\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_cdc083734cdc4132a52bc7f422446a81\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c2c11ff9f74a460b9956a7bb545986df\", \"value\": \"Downloading: 100%\"}}, \"811f80b7a9f24044aceecdf4f266f44e\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"61fb9b65c91c4d0792f441a009e5d9f6\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"f735ea5d804d4c5e9a5ced083c2a92c6\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_811f80b7a9f24044aceecdf4f266f44e\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_61fb9b65c91c4d0792f441a009e5d9f6\", \"value\": \" 29.0/29.0 [00:00&lt;00:00, 1.08kB/s]\"}}, \"778aefd716f74d50afe6e28d6c67c7ac\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"93ee287f85034c688772a906c5fb4cca\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_a194868856ff4fe28d0d07562e629d9c\", \"IPY_MODEL_34c3bdbdf0d14d73a835e2af601201f0\", \"IPY_MODEL_f735ea5d804d4c5e9a5ced083c2a92c6\"], \"layout\": \"IPY_MODEL_778aefd716f74d50afe6e28d6c67c7ac\"}}, \"6047475c76c243fcb04e46c6c958a840\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"80cb23fae4bf48c1947aad0b131f268c\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"29710991940541b3b40860d10f726c47\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6047475c76c243fcb04e46c6c958a840\", \"max\": 231508.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_80cb23fae4bf48c1947aad0b131f268c\", \"value\": 231508.0}}, \"6e38d4a7ab5548a4a46ce5ab438ad3be\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c4a274c5a97a46a0aa14e235cbfc58bb\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"c03ca09b53f548a7a63b3c26ad68acf7\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6e38d4a7ab5548a4a46ce5ab438ad3be\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c4a274c5a97a46a0aa14e235cbfc58bb\", \"value\": \"Downloading: 100%\"}}, \"c94e3f3ead4042c69fbc9a71af195042\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"293b42d4e03e497ebaf9fda528e17a83\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"2a310723a47146bea9ce36fee517c06b\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_c94e3f3ead4042c69fbc9a71af195042\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_293b42d4e03e497ebaf9fda528e17a83\", \"value\": \" 232k/232k [00:00&lt;00:00, 6.81MB/s]\"}}, \"00a8cde72aa54ce7b818132eb4ce34ed\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"03b6e5d14f7d465eaaa78aa9d214deef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_c03ca09b53f548a7a63b3c26ad68acf7\", \"IPY_MODEL_29710991940541b3b40860d10f726c47\", \"IPY_MODEL_2a310723a47146bea9ce36fee517c06b\"], \"layout\": \"IPY_MODEL_00a8cde72aa54ce7b818132eb4ce34ed\"}}, \"e034c175ca91439a9667ec6944b3aea1\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"3d41e96f25e74fc9b13c87551de084ae\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"e5faf04633ea49fb8848f9c747899701\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_e034c175ca91439a9667ec6944b3aea1\", \"max\": 466062.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_3d41e96f25e74fc9b13c87551de084ae\", \"value\": 466062.0}}, \"2271bd9873c648b48f5c2e364314bcea\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"fc8f530bd3c54783a842b4c446558849\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"d1175d9fdc7d4cc4839ef5ff436c9450\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_2271bd9873c648b48f5c2e364314bcea\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_fc8f530bd3c54783a842b4c446558849\", \"value\": \"Downloading: 100%\"}}, \"7d42f70bf85a45299da56dd97d35cedd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"87709494ef85440a9bb49f25512dc6f6\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"8306ee7ae4054b67856214de49ab058b\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_7d42f70bf85a45299da56dd97d35cedd\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_87709494ef85440a9bb49f25512dc6f6\", \"value\": \" 466k/466k [00:00&lt;00:00, 11.9MB/s]\"}}, \"9653d23b2ff04291ac96b6979ccc6c04\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"010f83be4f6f412487e749e86f75a56b\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d1175d9fdc7d4cc4839ef5ff436c9450\", \"IPY_MODEL_e5faf04633ea49fb8848f9c747899701\", \"IPY_MODEL_8306ee7ae4054b67856214de49ab058b\"], \"layout\": \"IPY_MODEL_9653d23b2ff04291ac96b6979ccc6c04\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Workflow Examples"},{"location":"workflow/#workflow-examples","text":"NERDA offers a simple easy-to-use interface for fine-tuning transformers for Named-Entity Recognition (=NER). We call this family of models NERDA models. NERDA can be used in two ways. You can either (1) train your own customized NERDA model or (2) download and use one of our precooked NERDA models for inference i.e. identifying named entities in new texts.","title":"Workflow Examples"},{"location":"workflow/#train-your-own-nerda-model","text":"We want to fine-tune a transformer for English. First, we download an English NER dataset CoNLL-2003 with annotated Named Entities, that we will use for training and evaluation of our model. from NERDA.datasets import get_conll_data, download_conll_data download_conll_data() Reading https://data.deepai.org/conll2003.zip 'archive extracted to /home/runner/.conll' CoNLL-2003 operates with the following types of named entities: PER sons ORG anizations LOC ations MISC ellaneous O utside (Not a named Entity) An observation from the CoNLL-2003 data set looks like this. # extract the first _5_ rows from the training and validation data splits. training = get_conll_data('train', 5) validation = get_conll_data('valid', 5) # example sentence = training.get('sentences')[0] tags = training.get('tags')[0] print(\"\\n\".join([\"{}/{}\".format(word, tag) for word, tag in zip(sentence, tags)])) EU/B-ORG rejects/O German/B-MISC call/O to/O boycott/O British/B-MISC lamb/O ./O If you provide your own dataset, it must have the same structure: It must be a dictionary The dictionary must contain 'sentences': a list of word-tokenized sentences with one sentence per entry 'tags': a list with the corresponding named-entity tags. The data set does however not have to follow the Inside-Outside-Beginning (IOB) tagging scheme 1 . The IOB tagging scheme implies, that words that are beginning of named entities are tagged with 'B-' and words 'inside' (=continuations of) named entities are tagged with 'I-' . That means that 'Joe Biden' should be tagged as Joe(B-PER) Biden(I-PER) . Now, instantiate a NERDA model for finetuning an ELECTRA transformer for NER. from NERDA.models import NERDA tag_scheme = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'] model = NERDA(dataset_training = training, dataset_validation = validation, tag_scheme = tag_scheme, tag_outside = 'O', transformer = 'google/electra-small-discriminator', hyperparameters = {'epochs' : 1, 'warmup_steps' : 10, 'train_batch_size': 5, 'learning_rate': 0.0001},) Device automatically set to: cpu var element = $('#08b62564-aab2-4f5c-be30-a694ce177498'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"09983c06919d462f83d7b70ac1da344b\"} var element = $('#b75870eb-209f-4b61-aae4-ed7155b40cd8'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"01e94a94d269403b844e06f5fad4e6d2\"} Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight'] - This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). var element = $('#c653245b-b312-4ad4-b5bc-7176b70e5822'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"93ee287f85034c688772a906c5fb4cca\"} var element = $('#cc70370a-4fd7-403c-b9e0-a612cc5d99d1'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"03b6e5d14f7d465eaaa78aa9d214deef\"} var element = $('#10b57c26-a02d-4768-803e-ed053512d3cd'); {\"version_major\": 2, \"version_minor\": 0, \"model_id\": \"010f83be4f6f412487e749e86f75a56b\"} Note, this model configuration only uses 5 sentences for model training to minimize execution time. Also the hyperparameters for the model have been chosen in order to minimize execution time. Therefore this example only serves to illustrate the functionality i.e. the resulting model will suck. By default the network architecture is analogous that of the models in Hvingelby et al. 2020 . The model can be trained right away by invoking the train method. model.train() Epoch 1 / 1 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.94it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.83it/s] Train Loss = 2.361093521118164 Valid Loss = 2.3556602001190186 'Model trained successfully' We can compute the performance of the model on a test set (limited to 5 sentences): test = get_conll_data('test', 5) model.evaluate_performance(test) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Level F1-Score Precision Recall 0 B-PER 0.0 0.0 0.0 1 I-PER 0.0 0.0 0.0 2 B-ORG 0.0 0.0 0.0 3 I-ORG 0.0 0.0 0.0 4 B-LOC 0.0 0.0 0.0 5 I-LOC 0.0 0.0 0.0 6 B-MISC 0.0 0.0 0.0 7 I-MISC 0.0 0.0 0.0 0 AVG_MICRO 0.0 NaN NaN 0 AVG_MICRO 0.0 NaN NaN Unsurprisingly, the model sucks in this case due to the ludicrous specification. Named Entities in new texts can be predicted with predict functions. text = \"Old MacDonald had a farm\" model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['I-PER', 'I-ORG', 'B-MISC', 'B-MISC', 'B-MISC']]) Needless to say the predicted entities for this model are nonsensical. To get a more reasonable model, provide more data and a more meaningful model specification. In general NERDA has the following handles, that you use. provide your own data set choose whatever pretrained transformer you would like to fine-tune provide your own set of hyperparameters and lastly provide your own torch network (architecture). You can do this by instantiating a NERDA model with the parameter 'network' set to your own network (torch.nn.Module).","title":"Train Your Own NERDA model"},{"location":"workflow/#use-a-precooked-nerda-model","text":"We have precooked a number of NERDA models, that you can download and use right off the shelf. Here is an example. Instantiate a NERDA model based on the English ELECTRA transformer, that has been finetuned for NER in English, EN_ELECTRA_EN . from NERDA.precooked import EN_ELECTRA_EN model = EN_ELECTRA_EN() Device automatically set to: cpu Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight'] - This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). (Down)load network: model.download_network() model.load_network() Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. Downloading https://nerda.s3-eu-west-1.amazonaws.com/EN_ELECTRA_EN.bin to /home/runner/.nerda/EN_ELECTRA_EN.bin 100% |########################################################################| Model loaded. Please make sure, that you're running the latest version of 'NERDA' otherwise the model is not guaranteed to work. This model performs much better: model.evaluate_performance(get_conll_data('test', 100)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Level F1-Score Precision Recall 0 B-PER 0.990909 1.000000 0.981982 1 I-PER 1.000000 1.000000 1.000000 2 B-ORG 0.800000 0.666667 1.000000 3 I-ORG 1.000000 1.000000 1.000000 4 B-LOC 0.993939 0.987952 1.000000 5 I-LOC 1.000000 1.000000 1.000000 6 B-MISC 0.930233 0.952381 0.909091 7 I-MISC 0.956522 1.000000 0.916667 0 AVG_MICRO 0.988060 NaN NaN 0 AVG_MICRO 0.958950 NaN NaN Predict named entities in new texts text = 'Old MacDonald had a farm' model.predict_text(text) ([['Old', 'MacDonald', 'had', 'a', 'farm']], [['B-PER', 'I-PER', 'O', 'O', 'O']])","title":"Use a Precooked NERDA model"},{"location":"workflow/#list-of-precooked-models","text":"The table below shows the precooked NERDA models publicly available for download. We have trained models for Danish and English. Model Language Transformer Dataset F1-score DA_BERT_ML Danish Multilingual BERT DaNE 82.8 DA_ELECTRA_DA Danish Danish ELECTRA DaNE 79.8 EN_BERT_ML English Multilingual BERT CoNLL-2003 90.4 EN_ELECTRA_EN English English ELECTRA CoNLL-2003 89.1 F1-score is the micro-averaged F1-score across entity tags and is evaluated on the respective test sets (that have not been used for training nor validation of the models). Note, that we have not spent a lot of time on actually fine-tuning the models, so there could be room for improvement. If you are able to improve the models, we will be happy to hear from you and include your NERDA model.","title":"List of Precooked Models"},{"location":"workflow/#performance-of-precooked-models","text":"The table below summarizes the performance as measured by F1-scores of the model configurations, that NERDA ships with. Level DA_BERT_ML DA_ELECTRA_DA EN_BERT_ML EN_ELECTRA_EN B-PER 93.8 92.0 96.0 95.1 I-PER 97.8 97.1 98.5 97.9 B-ORG 69.5 66.9 88.4 86.2 I-ORG 69.9 70.7 85.7 83.1 B-LOC 82.5 79.0 92.3 91.1 I-LOC 31.6 44.4 83.9 80.5 B-MISC 73.4 68.6 81.8 80.1 I-MISC 86.1 63.6 63.4 68.4 AVG_MICRO 82.8 79.8 90.4 89.1 AVG_MACRO 75.6 72.8 86.3 85.3 This concludes our walkthrough of NERDA . If you have any questions, please do not hesitate to contact us ! {\"state\": {\"305e4c6914644c18a13d6dd6d7bd5a55\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"3237c4384da94f139d3091d3d0bacf5b\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c4039a63550a45ffb1450f72acf95830\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_305e4c6914644c18a13d6dd6d7bd5a55\", \"max\": 665.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_3237c4384da94f139d3091d3d0bacf5b\", \"value\": 665.0}}, \"33599426d91645918d01134477344962\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8dc6711db10046f6b1cb035e72c9a2a2\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"32122a5509db462ab09448e37f496336\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_33599426d91645918d01134477344962\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_8dc6711db10046f6b1cb035e72c9a2a2\", \"value\": \"Downloading: 100%\"}}, \"664d15ae485d42bc90f2783f18f8ad53\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"9669c61537a442268e980208098eb2c5\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"91e4954d53124e5a9b5ca7a3f16482a4\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_664d15ae485d42bc90f2783f18f8ad53\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_9669c61537a442268e980208098eb2c5\", \"value\": \" 665/665 [00:00&lt;00:00, 24.8kB/s]\"}}, \"84f10b91e95445c091b10186b2246b70\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"09983c06919d462f83d7b70ac1da344b\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_32122a5509db462ab09448e37f496336\", \"IPY_MODEL_c4039a63550a45ffb1450f72acf95830\", \"IPY_MODEL_91e4954d53124e5a9b5ca7a3f16482a4\"], \"layout\": \"IPY_MODEL_84f10b91e95445c091b10186b2246b70\"}}, \"aa6e3ddcff7c462c86e7d9d8683149f6\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"435348c4408d4d3eab8746badcfafb21\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c41dc83e16c94d30a5817e3092569b5d\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_aa6e3ddcff7c462c86e7d9d8683149f6\", \"max\": 54245363.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_435348c4408d4d3eab8746badcfafb21\", \"value\": 54245363.0}}, \"c1818ad0151649978aea49edb40acd20\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"354fa859b1054490b7799b44f1378fc8\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"631c29019fc043eeb3d5152deaafc1af\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_c1818ad0151649978aea49edb40acd20\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_354fa859b1054490b7799b44f1378fc8\", \"value\": \"Downloading: 100%\"}}, \"23aaf4b17bb24cc791d05ea8c48e36f7\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"07d1cdb334944f70bccfe63d6152e580\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"45350bd1d66a42a790e1a9c2c2d035a1\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_23aaf4b17bb24cc791d05ea8c48e36f7\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_07d1cdb334944f70bccfe63d6152e580\", \"value\": \" 54.2M/54.2M [00:01&lt;00:00, 24.6MB/s]\"}}, \"1c9c8deecdd5408eb42e9e726057010b\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"01e94a94d269403b844e06f5fad4e6d2\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_631c29019fc043eeb3d5152deaafc1af\", \"IPY_MODEL_c41dc83e16c94d30a5817e3092569b5d\", \"IPY_MODEL_45350bd1d66a42a790e1a9c2c2d035a1\"], \"layout\": \"IPY_MODEL_1c9c8deecdd5408eb42e9e726057010b\"}}, \"87ddfd9b1bb74b92b242e901a53c44e3\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"97c62537400143ff9831d716f839877f\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"34c3bdbdf0d14d73a835e2af601201f0\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_87ddfd9b1bb74b92b242e901a53c44e3\", \"max\": 29.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_97c62537400143ff9831d716f839877f\", \"value\": 29.0}}, \"cdc083734cdc4132a52bc7f422446a81\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c2c11ff9f74a460b9956a7bb545986df\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"a194868856ff4fe28d0d07562e629d9c\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_cdc083734cdc4132a52bc7f422446a81\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c2c11ff9f74a460b9956a7bb545986df\", \"value\": \"Downloading: 100%\"}}, \"811f80b7a9f24044aceecdf4f266f44e\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"61fb9b65c91c4d0792f441a009e5d9f6\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"f735ea5d804d4c5e9a5ced083c2a92c6\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_811f80b7a9f24044aceecdf4f266f44e\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_61fb9b65c91c4d0792f441a009e5d9f6\", \"value\": \" 29.0/29.0 [00:00&lt;00:00, 1.08kB/s]\"}}, \"778aefd716f74d50afe6e28d6c67c7ac\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"93ee287f85034c688772a906c5fb4cca\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_a194868856ff4fe28d0d07562e629d9c\", \"IPY_MODEL_34c3bdbdf0d14d73a835e2af601201f0\", \"IPY_MODEL_f735ea5d804d4c5e9a5ced083c2a92c6\"], \"layout\": \"IPY_MODEL_778aefd716f74d50afe6e28d6c67c7ac\"}}, \"6047475c76c243fcb04e46c6c958a840\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"80cb23fae4bf48c1947aad0b131f268c\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"29710991940541b3b40860d10f726c47\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6047475c76c243fcb04e46c6c958a840\", \"max\": 231508.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_80cb23fae4bf48c1947aad0b131f268c\", \"value\": 231508.0}}, \"6e38d4a7ab5548a4a46ce5ab438ad3be\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c4a274c5a97a46a0aa14e235cbfc58bb\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"c03ca09b53f548a7a63b3c26ad68acf7\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_6e38d4a7ab5548a4a46ce5ab438ad3be\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c4a274c5a97a46a0aa14e235cbfc58bb\", \"value\": \"Downloading: 100%\"}}, \"c94e3f3ead4042c69fbc9a71af195042\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"293b42d4e03e497ebaf9fda528e17a83\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"2a310723a47146bea9ce36fee517c06b\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_c94e3f3ead4042c69fbc9a71af195042\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_293b42d4e03e497ebaf9fda528e17a83\", \"value\": \" 232k/232k [00:00&lt;00:00, 6.81MB/s]\"}}, \"00a8cde72aa54ce7b818132eb4ce34ed\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"03b6e5d14f7d465eaaa78aa9d214deef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_c03ca09b53f548a7a63b3c26ad68acf7\", \"IPY_MODEL_29710991940541b3b40860d10f726c47\", \"IPY_MODEL_2a310723a47146bea9ce36fee517c06b\"], \"layout\": \"IPY_MODEL_00a8cde72aa54ce7b818132eb4ce34ed\"}}, \"e034c175ca91439a9667ec6944b3aea1\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"3d41e96f25e74fc9b13c87551de084ae\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"e5faf04633ea49fb8848f9c747899701\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_e034c175ca91439a9667ec6944b3aea1\", \"max\": 466062.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_3d41e96f25e74fc9b13c87551de084ae\", \"value\": 466062.0}}, \"2271bd9873c648b48f5c2e364314bcea\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"fc8f530bd3c54783a842b4c446558849\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"d1175d9fdc7d4cc4839ef5ff436c9450\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_2271bd9873c648b48f5c2e364314bcea\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_fc8f530bd3c54783a842b4c446558849\", \"value\": \"Downloading: 100%\"}}, \"7d42f70bf85a45299da56dd97d35cedd\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"87709494ef85440a9bb49f25512dc6f6\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"8306ee7ae4054b67856214de49ab058b\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_7d42f70bf85a45299da56dd97d35cedd\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_87709494ef85440a9bb49f25512dc6f6\", \"value\": \" 466k/466k [00:00&lt;00:00, 11.9MB/s]\"}}, \"9653d23b2ff04291ac96b6979ccc6c04\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"010f83be4f6f412487e749e86f75a56b\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d1175d9fdc7d4cc4839ef5ff436c9450\", \"IPY_MODEL_e5faf04633ea49fb8848f9c747899701\", \"IPY_MODEL_8306ee7ae4054b67856214de49ab058b\"], \"layout\": \"IPY_MODEL_9653d23b2ff04291ac96b6979ccc6c04\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Performance of Precooked Models"}]}